# üìã S1-010: LLM Call Logging

## –ú–µ—Ç–∞

–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–æ–∂–Ω–æ–≥–æ LLM-–≤–∏–∫–ª–∏–∫—É –≤ —Ç–∞–±–ª–∏—Ü—é `llm_calls` (action, strategy, provider, model, tokens, latency, cost, success/fail). –§–∞–±—Ä–∏–∫–∞ `create_model_router()` –¥–ª—è –∑–±–∏—Ä–∞–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ —Å—Ç–µ–∫—É –≤ –æ–¥–∏–Ω –≤–∏–∫–ª–∏–∫.

## –ö–æ–Ω—Ç–µ–∫—Å—Ç

–ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ S1-005 (DB/Alembic ‚Äî —Ç–∞–±–ª–∏—Ü—è `llm_calls`), S1-007 (providers), S1-008 (registry), S1-009 (router + LogCallback). –ó–∞–≤–µ—Ä—à—É—î LLM-—ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É. –ü—ñ—Å–ª—è —Ü—å–æ–≥–æ —Ç–∞—Å–∫—É ‚Äî –±—É–¥—å-—è–∫–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –º–æ–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ `create_model_router()`.

---

## Acceptance Criteria

- [ ] `create_log_callback(session_factory)` –ø–æ–≤–µ—Ä—Ç–∞—î async callback —Å—É–º—ñ—Å–Ω–∏–π –∑ `LogCallback`
- [ ] Callback –∑–±–µ—Ä—ñ–≥–∞—î `LLMCall` –∑–∞–ø–∏—Å –ø—Ä–∏ success (–∑ tokens, cost, latency)
- [ ] Callback –∑–±–µ—Ä—ñ–≥–∞—î `LLMCall` –∑–∞–ø–∏—Å –ø—Ä–∏ failure (–∑ error_message)
- [ ] `action` —Ç–∞ `strategy` –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ –∫–æ–∂–Ω–æ–º—É –∑–∞–ø–∏—Å—ñ
- [ ] DB-–ø–æ–º–∏–ª–∫–∏ swallowed (logged, –Ω–µ raised) ‚Äî logging –Ω—ñ–∫–æ–ª–∏ –Ω–µ –ª–∞–º–∞—î pipeline
- [ ] `create_model_router(settings, session_factory)` ‚Äî one-stop factory
- [ ] Unit-—Ç–µ—Å—Ç–∏ –∑ mock session

---

## –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

### src/course_supporter/llm/logging.py

```python
"""LLM call logging ‚Äî async callback for ModelRouter.

Persists every LLM call to llm_calls table. DB errors are swallowed
(logged via structlog) to never break the main pipeline.
"""

from collections.abc import Awaitable, Callable

import structlog
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker

from course_supporter.llm.schemas import LLMResponse

logger = structlog.get_logger()

LogCallback = Callable[[LLMResponse | None, Exception | None], Awaitable[None]]


def create_log_callback(
    session_factory: async_sessionmaker[AsyncSession],
) -> LogCallback:
    """Create async log callback that persists LLM calls to DB.

    Each call gets its own session ‚Äî isolated from business transactions.
    DB errors are swallowed and logged, never raised.
    """

    async def _log_callback(
        response: LLMResponse | None,
        error: Exception | None,
    ) -> None:
        try:
            async with session_factory() as session:
                from course_supporter.db.models import LLMCall

                record = LLMCall(
                    action=response.action if response else "",
                    strategy=response.strategy if response else "default",
                    provider=response.provider if response else "unknown",
                    model_id=response.model_id if response else "unknown",
                    tokens_in=response.tokens_in if response else None,
                    tokens_out=response.tokens_out if response else None,
                    latency_ms=response.latency_ms if response else 0,
                    cost_usd=response.cost_usd if response else None,
                    success=error is None,
                    error_message=str(error) if error else None,
                )
                session.add(record)
                await session.commit()

                logger.debug(
                    "llm_call_logged",
                    action=record.action,
                    strategy=record.strategy,
                    provider=record.provider,
                    success=record.success,
                )

        except Exception as exc:
            logger.error(
                "llm_call_log_failed",
                error=str(exc),
                original_error=str(error) if error else None,
            )

    return _log_callback
```

### src/course_supporter/llm/setup.py

```python
"""One-stop factory for assembling the full LLM stack.

Usage:
    router = create_model_router(settings, session_factory)
    response = await router.complete("video_analysis", prompt)
"""

import structlog
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker

from course_supporter.config import Settings
from course_supporter.llm.factory import create_providers
from course_supporter.llm.logging import create_log_callback
from course_supporter.llm.registry import load_registry
from course_supporter.llm.router import ModelRouter

logger = structlog.get_logger()


def create_model_router(
    settings: Settings,
    session_factory: async_sessionmaker[AsyncSession] | None = None,
    *,
    fallback_strategy: str | None = None,
    max_retries: int = 2,
) -> ModelRouter:
    """Assemble complete LLM stack: registry + providers + logging + router.

    Args:
        settings: application settings with API keys
        session_factory: DB session factory for logging (optional)
        fallback_strategy: cross-strategy fallback (e.g., "budget")
        max_retries: retries per model on transient errors

    Returns:
        Configured ModelRouter ready for use.
    """
    registry = load_registry()
    logger.info(
        "model_registry_loaded",
        models=len(registry.models),
        actions=len(registry.actions),
    )

    providers = create_providers(settings)

    log_callback = None
    if session_factory is not None:
        log_callback = create_log_callback(session_factory)

    router = ModelRouter(
        registry=registry,
        providers=providers,
        log_callback=log_callback,
        max_retries=max_retries,
        fallback_strategy=fallback_strategy,
    )
    logger.info(
        "model_router_created",
        providers=list(providers.keys()),
        fallback_strategy=fallback_strategy,
    )

    return router
```

### src/course_supporter/llm/__init__.py

```python
"""LLM infrastructure: providers, registry, router, logging.

Quick start:
    from course_supporter.llm import create_model_router

    router = create_model_router(settings, session_factory)
    response = await router.complete("video_analysis", prompt)
    response = await router.complete("course_structuring", prompt, strategy="quality")
"""

from course_supporter.llm.router import AllModelsFailedError, ModelRouter
from course_supporter.llm.setup import create_model_router

__all__ = [
    "AllModelsFailedError",
    "ModelRouter",
    "create_model_router",
]
```

---

## ORM –º–æ–¥–µ–ª—å (–æ–Ω–æ–≤–ª–µ–Ω–Ω—è)

–¢–∞–±–ª–∏—Ü—è `llm_calls` (—Å—Ç–≤–æ—Ä–µ–Ω–∞ –≤ S1-005) –ø–æ—Ç—Ä–µ–±—É—î –¥–≤–∞ –Ω–æ–≤–∏—Ö –ø–æ–ª—è: `action` —Ç–∞ `strategy`.

```python
class LLMCall(Base):
    """Record of every LLM API call."""

    __tablename__ = "llm_calls"

    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid7)
    action: Mapped[str] = mapped_column(String(100), default="")
    strategy: Mapped[str] = mapped_column(String(50), default="default")
    provider: Mapped[str] = mapped_column(String(50))
    model_id: Mapped[str] = mapped_column(String(100))
    tokens_in: Mapped[int | None] = mapped_column(default=None)
    tokens_out: Mapped[int | None] = mapped_column(default=None)
    latency_ms: Mapped[int] = mapped_column(default=0)
    cost_usd: Mapped[float | None] = mapped_column(default=None)
    success: Mapped[bool] = mapped_column(default=True)
    error_message: Mapped[str | None] = mapped_column(Text, default=None)
    created_at: Mapped[datetime] = mapped_column(server_default=func.now())
```

**Alembic migration**: –¥–æ–¥–∞—Ç–∏ `action` —Ç–∞ `strategy` –¥–æ —ñ—Å–Ω—É—é—á–æ—ó —Ç–∞–±–ª–∏—Ü—ñ.

---

## –¢–µ—Å—Ç–∏

### tests/unit/test_llm/test_logging.py

```python
"""Tests for LLM call logging."""

from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from course_supporter.llm.logging import create_log_callback
from course_supporter.llm.schemas import LLMResponse


class TestLogCallback:
    @pytest.mark.anyio
    async def test_logs_success(self) -> None:
        mock_session = AsyncMock()
        mock_factory = MagicMock(return_value=mock_session)
        mock_session.__aenter__ = AsyncMock(return_value=mock_session)
        mock_session.__aexit__ = AsyncMock(return_value=None)

        callback = create_log_callback(mock_factory)

        response = LLMResponse(
            content="ok",
            provider="gemini",
            model_id="gemini-2.5-flash",
            tokens_in=100,
            tokens_out=50,
            latency_ms=200,
            cost_usd=0.001,
            action="video_analysis",
            strategy="default",
        )

        await callback(response, None)
        mock_session.add.assert_called_once()
        mock_session.commit.assert_called_once()

    @pytest.mark.anyio
    async def test_logs_failure(self) -> None:
        mock_session = AsyncMock()
        mock_factory = MagicMock(return_value=mock_session)
        mock_session.__aenter__ = AsyncMock(return_value=mock_session)
        mock_session.__aexit__ = AsyncMock(return_value=None)

        callback = create_log_callback(mock_factory)

        await callback(None, RuntimeError("API down"))
        mock_session.add.assert_called_once()

    @pytest.mark.anyio
    async def test_db_error_swallowed(self) -> None:
        mock_session = AsyncMock()
        mock_factory = MagicMock(return_value=mock_session)
        mock_session.__aenter__ = AsyncMock(return_value=mock_session)
        mock_session.__aexit__ = AsyncMock(return_value=None)
        mock_session.commit = AsyncMock(
            side_effect=RuntimeError("DB connection lost")
        )

        callback = create_log_callback(mock_factory)
        response = LLMResponse(content="ok", provider="test", model_id="test")

        # Should not raise
        await callback(response, None)


class TestCreateModelRouter:
    def test_creates_router(self) -> None:
        from course_supporter.config import Settings
        from course_supporter.llm.setup import create_model_router

        s = Settings(gemini_api_key="test-key", _env_file=None)  # type: ignore[arg-type]

        with patch(
            "course_supporter.llm.setup.load_registry"
        ) as mock_load:
            from course_supporter.llm.registry import ModelRegistryConfig

            mock_load.return_value = ModelRegistryConfig.model_validate({
                "models": {
                    "m": {
                        "provider": "gemini",
                        "capabilities": ["structured_output"],
                        "max_context": 100000,
                        "cost_per_1k": {"input": 0.001, "output": 0.002},
                    }
                },
                "actions": {
                    "test": {
                        "description": "t",
                        "requires": ["structured_output"],
                    }
                },
                "routing": {"test": {"default": ["m"]}},
            })

            router = create_model_router(s)
            assert router is not None
```

---

## –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ Epic 2

```
src/course_supporter/llm/
‚îú‚îÄ‚îÄ __init__.py               # Public API: ModelRouter, create_model_router
‚îú‚îÄ‚îÄ schemas.py                # LLMRequest, LLMResponse
‚îú‚îÄ‚îÄ factory.py                # create_providers()
‚îú‚îÄ‚îÄ registry.py               # ModelRegistryConfig, load_registry()
‚îú‚îÄ‚îÄ router.py                 # ModelRouter, AllModelsFailedError
‚îú‚îÄ‚îÄ logging.py                # create_log_callback()
‚îú‚îÄ‚îÄ setup.py                  # create_model_router() ‚Äî one-stop factory
‚îî‚îÄ‚îÄ providers/
    ‚îú‚îÄ‚îÄ __init__.py            # PROVIDER_REGISTRY
    ‚îú‚îÄ‚îÄ base.py                # LLMProvider ABC + enable/disable
    ‚îú‚îÄ‚îÄ gemini.py
    ‚îú‚îÄ‚îÄ anthropic.py
    ‚îî‚îÄ‚îÄ openai_compat.py

config/
    models.yaml               # models + actions + routing

tests/unit/test_llm/
    __init__.py
    test_providers.py
    test_registry.py
    test_router.py
    test_logging.py
```

---

## –ö—Ä–æ–∫–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è

1. –û–Ω–æ–≤–∏—Ç–∏ `db/models.py` ‚Äî –¥–æ–¥–∞—Ç–∏ `action`, `strategy` –¥–æ `LLMCall`
2. –°—Ç–≤–æ—Ä–∏—Ç–∏ Alembic migration
3. –°—Ç–≤–æ—Ä–∏—Ç–∏ `llm/logging.py`
4. –°—Ç–≤–æ—Ä–∏—Ç–∏ `llm/setup.py`
5. –°—Ç–≤–æ—Ä–∏—Ç–∏ `llm/__init__.py`
6. –°—Ç–≤–æ—Ä–∏—Ç–∏ `tests/unit/test_llm/test_logging.py`
7. `make check`

---

## –ü—Ä–∏–º—ñ—Ç–∫–∏

- **–û–∫—Ä–µ–º–∞ session** ‚Äî callback —Å—Ç–≤–æ—Ä—é—î —Å–≤–æ—é session. –Ü–∑–æ–ª—è—Ü—ñ—è –≤—ñ–¥ –±—ñ–∑–Ω–µ—Å-—Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π.
- **Import inside callback** ‚Äî `from course_supporter.db.models import LLMCall` inside function, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ circular imports.
- **`create_model_router()`** ‚Äî —î–¥–∏–Ω–∏–π entry point. –ó–Ω–∞—î –ø—Ä–æ –≤—Å—ñ —à–∞—Ä–∏, —Ä–µ—à—Ç–∞ –∫–æ–¥—É –∑–Ω–∞—î —Ç—ñ–ª—å–∫–∏ ModelRouter.
- **–ë–µ–∑ session_factory** ‚Äî router –ø—Ä–∞—Ü—é—î –±–µ–∑ –ª–æ–≥—É–≤–∞–Ω–Ω—è. –ó—Ä—É—á–Ω–æ –¥–ª—è —Ç–µ—Å—Ç—ñ–≤ —Ç–∞ CLI.
