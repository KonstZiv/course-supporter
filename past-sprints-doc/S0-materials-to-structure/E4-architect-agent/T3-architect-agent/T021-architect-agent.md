# üìã S1-021: ArchitectAgent Class

## –ú–µ—Ç–∞

–†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `ArchitectAgent` ‚Äî –∫–ª—é—á–æ–≤–∏–π –∫–ª–∞—Å, —è–∫–∏–π –ø—Ä–∏–π–º–∞—î `CourseContext`, —Å–µ—Ä—ñ–∞–ª—ñ–∑—É—î –π–æ–≥–æ, —Ñ–æ—Ä–º–∞—Ç—É—î –ø—Ä–æ–º–ø—Ç, –≤–∏–∫–ª–∏–∫–∞—î LLM —á–µ—Ä–µ–∑ `ModelRouter.complete_structured()` —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –≤–∞–ª—ñ–¥–Ω—É `CourseStructure`. –¶–µ —è–¥—Ä–æ –±—ñ–∑–Ω–µ—Å-–ª–æ–≥—ñ–∫–∏ –ø—Ä–æ—î–∫—Ç—É.

## –ö–æ–Ω—Ç–µ–∫—Å—Ç

–¢—Ä–µ—Ç—è –∑–∞–¥–∞—á–∞ Epic 4. –ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ S1-019 (CourseStructure ‚Äî response schema) —Ç–∞ S1-020 (prompt_loader ‚Äî system/user prompt). –ë–ª–æ–∫—É—î S1-022 (persistence ‚Äî –∑–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç). –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î `action="course_structuring"` –∑ `config/models.yaml` (–≤–∂–µ –≤–∏–∑–Ω–∞—á–µ–Ω–∏–π: default chain `gemini-2.5-flash ‚Üí deepseek-chat`, quality chain `claude-sonnet ‚Üí gemini-2.5-pro`).

–§–∞–π–ª `agents/architect.py` –≤–∂–µ —ñ—Å–Ω—É—î —è–∫ stub (TODO) ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–∞–º—ñ–Ω–∏—Ç–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω–∏–π –∫–æ–¥.

---

## –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è: Step-Based Design

### –ú–æ—Ç–∏–≤–∞—Ü—ñ—è

–ú–æ–Ω–æ–ª—ñ—Ç–Ω–∏–π `run()` –∑ —î–¥–∏–Ω–∏–º LLM-–≤–∏–∫–ª–∏–∫–æ–º –¥–æ—Å—Ç–∞—Ç–Ω—ñ–π –¥–ª—è MVP, –∞–ª–µ —Å—Ç–≤–æ—Ä—é—î –ø—Ä–æ–±–ª–µ–º–∏ –ø—Ä–∏ –º—ñ–≥—Ä–∞—Ü—ñ—ó –Ω–∞ –ª–∞–Ω—Ü—é–≥–∏/–≥—Ä–∞—Ñ–∏ (LangGraph, custom DAG):

- –ù–µ –º–æ–∂–Ω–∞ –≤—Å—Ç–∞–≤–∏—Ç–∏ hook –º—ñ–∂ –∫—Ä–æ–∫–∞–º–∏ (–ª–æ–≥—É–≤–∞–Ω–Ω—è, –ª—é–¥—Å—å–∫–∏–π —Ñ—ñ–¥–±–µ–∫, –≤–∞–ª—ñ–¥–∞—Ü—ñ—è)
- –ù–µ–º–∞—î –ø—Ä–æ–º—ñ–∂–Ω–æ–≥–æ —Å—Ç–∞–Ω—É –¥–ª—è streaming –∞–±–æ —á–∞—Å—Ç–∫–æ–≤–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
- –ù–µ–º–æ–∂–ª–∏–≤–æ —Ä–æ–∑–±–∏—Ç–∏ –Ω–∞ –Ω–æ–¥–∏ –≥—Ä–∞—Ñ–∞ –±–µ–∑ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É

### –†—ñ—à–µ–Ω–Ω—è

–†–æ–∑–±–∏—Ç–∏ `run()` –Ω–∞ **–æ–∫—Ä–µ–º—ñ –º–µ—Ç–æ–¥–∏-–∫—Ä–æ–∫–∏** –∑ –ø—Ä–æ–º—ñ–∂–Ω–∏–º —Ç–∏–ø–æ–º `PreparedPrompt`:

```
run(context)
  ‚îú‚îÄ _prepare_prompts(context) ‚Üí PreparedPrompt    # step 1: load & format
  ‚îî‚îÄ _generate(prepared)       ‚Üí CourseStructure   # step 2: call LLM
```

### –ü–µ—Ä–µ–≤–∞–≥–∏

1. **–ö–æ–∂–µ–Ω –º–µ—Ç–æ–¥ ‚Äî –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∞ –Ω–æ–¥–∞** –≤ LangGraph –∞–±–æ step —É –ª–∞–Ω—Ü—é–≥—É
2. **–ü—Ä–æ–º—ñ–∂–Ω—ñ —Ç–∏–ø–∏** (`PreparedPrompt`) —Å—Ç–∞—é—Ç—å —á–∞—Å—Ç–∏–Ω–æ—é State –≥—Ä–∞—Ñ–∞
3. **–õ–µ–≥–∫–æ –¥–æ–¥–∞—Ç–∏ –∫—Ä–æ–∫–∏**: `_validate()`, `_refine()`, `_chunk()` –±–µ–∑ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É
4. **Per-step —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è**: –º–æ–∂–Ω–∞ —Ç–µ—Å—Ç—É–≤–∞—Ç–∏ prompt formatting –æ–∫—Ä–µ–º–æ –≤—ñ–¥ LLM
5. **–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π overhead**: –¥–ª—è MVP ‚Äî –ª—ñ–Ω—ñ–π–Ω–∏–π –≤–∏–∫–ª–∏–∫, –∞–ª–µ –≥–æ—Ç–æ–≤–∏–π –¥–æ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è

### –ú—ñ–≥—Ä–∞—Ü—ñ–π–Ω–∏–π —à–ª—è—Ö (Epic 7+)

```
# –°—å–æ–≥–æ–¥–Ω—ñ (MVP): –ª—ñ–Ω—ñ–π–Ω–∏–π –≤–∏–∫–ª–∏–∫
run() ‚Üí _prepare_prompts() ‚Üí _generate() ‚Üí return

# –ó–∞–≤—Ç—Ä–∞ (LangGraph): –∫–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —Å—Ç–∞—î –Ω–æ–¥–æ—é
START ‚Üí prepare_prompts_node ‚Üí generate_node ‚Üí validate_node ‚Üí END
                                    ‚Üë                  |
                                    ‚îî‚îÄ‚îÄ retry_edge ‚Üê‚îÄ‚îÄ‚îÄ‚îò

# –ó–∞–≤—Ç—Ä–∞ (multi-step): —Ä–æ–∑–±–∏—Ç–∏ _generate –Ω–∞ sub-steps
_generate_modules() ‚Üí _generate_lessons(per module) ‚Üí _generate_exercises()
```

---

## Acceptance Criteria

- [ ] `ArchitectAgent.__init__` –ø—Ä–∏–π–º–∞—î `router`, `prompt_path`, `strategy`, `temperature`, `max_tokens`
- [ ] `ArchitectAgent.run(context: CourseContext) -> CourseStructure` ‚Äî async orchestrator
- [ ] `_prepare_prompts(context) -> PreparedPrompt` ‚Äî sync, load YAML + serialize context
- [ ] `_generate(prepared: PreparedPrompt) -> CourseStructure` ‚Äî async, call router
- [ ] `PreparedPrompt` ‚Äî NamedTuple –∑ system_prompt, user_prompt, prompt_version
- [ ] –í–∏–∫–ª–∏–∫–∞—î `router.complete_structured(action="course_structuring", response_schema=CourseStructure)`
- [ ] –ü–æ–≤–µ—Ä—Ç–∞—î –ø–µ—Ä—à–∏–π –µ–ª–µ–º–µ–Ω—Ç tuple (parsed CourseStructure)
- [ ] –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ—Ç `AllModelsFailedError` –≤—ñ–¥ router (–Ω–µ –≥–ª—É—à–∏—Ç—å)
- [ ] Default prompt path: `prompts/architect/v1.yaml`
- [ ] ~12 unit-—Ç–µ—Å—Ç—ñ–≤ –∑ mocked router, –≤—Å—ñ –∑–µ–ª–µ–Ω—ñ (–≤–∫–ª—é—á–∞—é—á–∏ per-step —Ç–µ—Å—Ç–∏)
- [ ] `make check` –ø—Ä–æ—Ö–æ–¥–∏—Ç—å

---

## –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

### src/course_supporter/agents/architect.py

```python
"""ArchitectAgent: generates course structure from materials via LLM."""

from typing import NamedTuple

import structlog

from course_supporter.agents.prompt_loader import format_user_prompt, load_prompt
from course_supporter.llm.router import ModelRouter
from course_supporter.models.course import CourseContext, CourseStructure

logger = structlog.get_logger()

DEFAULT_PROMPT_PATH = "prompts/architect/v1.yaml"


class PreparedPrompt(NamedTuple):
    """Intermediate result of prompt preparation step.

    Separates prompt loading/formatting from LLM invocation,
    enabling independent testing and future graph-based orchestration
    where each step becomes a node.
    """

    system_prompt: str
    user_prompt: str
    prompt_version: str


class ArchitectAgent:
    """Generates structured course program from course materials.

    Uses ModelRouter with action='course_structuring' to call LLM
    with structured output (CourseStructure Pydantic schema).

    Architecture: step-based design for future chain/graph migration.
    Each step is a separate method that can become a node in a
    LangGraph or custom DAG pipeline.

    Steps:
        1. _prepare_prompts: load YAML template, serialize context, format prompt
        2. _generate: call LLM via ModelRouter, return validated structure

    Args:
        router: ModelRouter instance for LLM calls.
        prompt_path: Path to YAML prompt template.
        strategy: Routing strategy ('default', 'quality', 'budget').
        temperature: LLM temperature (0.0 = deterministic).
        max_tokens: Maximum output tokens.
    """

    def __init__(
        self,
        router: ModelRouter,
        *,
        prompt_path: str = DEFAULT_PROMPT_PATH,
        strategy: str = "default",
        temperature: float = 0.0,
        max_tokens: int = 8192,
    ) -> None:
        self._router = router
        self._prompt_path = prompt_path
        self._strategy = strategy
        self._temperature = temperature
        self._max_tokens = max_tokens

    async def run(self, context: CourseContext) -> CourseStructure:
        """Generate course structure from materials.

        Orchestrates the pipeline: prepare prompts ‚Üí generate via LLM.

        Args:
            context: Unified course context from ingestion pipeline.

        Returns:
            Validated CourseStructure from LLM.

        Raises:
            AllModelsFailedError: If all models in all strategies fail.
            FileNotFoundError: If prompt file not found.
        """
        prepared = self._prepare_prompts(context)
        return await self._generate(prepared, documents_count=len(context.documents))

    def _prepare_prompts(self, context: CourseContext) -> PreparedPrompt:
        """Step 1: Load prompt template and format with context.

        Loads YAML prompt file, serializes CourseContext to JSON,
        and injects it into the user prompt template.

        Args:
            context: Course context to serialize into the prompt.

        Returns:
            PreparedPrompt with system prompt, formatted user prompt,
            and prompt version for logging/A/B testing.

        Raises:
            FileNotFoundError: If prompt YAML file not found.
            KeyError: If YAML missing required keys.
        """
        prompt_data = load_prompt(self._prompt_path)
        system_prompt = prompt_data["system_prompt"]
        user_prompt = format_user_prompt(
            prompt_data["user_prompt_template"],
            context.model_dump_json(indent=2),
        )
        return PreparedPrompt(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            prompt_version=prompt_data.get("version", "unknown"),
        )

    async def _generate(
        self,
        prepared: PreparedPrompt,
        *,
        documents_count: int = 0,
    ) -> CourseStructure:
        """Step 2: Call LLM and return validated CourseStructure.

        Sends prepared prompts to ModelRouter with structured output
        and returns the parsed Pydantic model.

        Args:
            prepared: Formatted prompts from _prepare_prompts step.
            documents_count: Number of documents for logging.

        Returns:
            Validated CourseStructure from LLM.

        Raises:
            AllModelsFailedError: If all models in all strategies fail.
        """
        logger.info(
            "architect_agent_generating",
            strategy=self._strategy,
            prompt_version=prepared.prompt_version,
            documents_count=documents_count,
            context_length=len(prepared.user_prompt),
        )

        structure, response = await self._router.complete_structured(
            action="course_structuring",
            prompt=prepared.user_prompt,
            response_schema=CourseStructure,
            system_prompt=prepared.system_prompt,
            temperature=self._temperature,
            max_tokens=self._max_tokens,
            strategy=self._strategy,
        )

        logger.info(
            "architect_agent_done",
            modules_count=len(structure.modules),
            model=response.model_id,
            tokens_in=response.tokens_in,
            tokens_out=response.tokens_out,
            cost_usd=response.cost_usd,
        )

        return structure
```

### src/course_supporter/agents/__init__.py (–æ–Ω–æ–≤–∏—Ç–∏)

```python
"""Agents for course structure generation."""

from course_supporter.agents.architect import ArchitectAgent, PreparedPrompt
from course_supporter.agents.prompt_loader import format_user_prompt, load_prompt

__all__ = [
    "ArchitectAgent",
    "PreparedPrompt",
    "format_user_prompt",
    "load_prompt",
]
```

---

## –¢–µ—Å—Ç–∏

### tests/unit/test_architect_agent.py

```python
"""Tests for ArchitectAgent."""

from typing import Any
from unittest.mock import AsyncMock, patch

import pytest

from course_supporter.agents.architect import (
    DEFAULT_PROMPT_PATH,
    ArchitectAgent,
    PreparedPrompt,
)
from course_supporter.llm.router import AllModelsFailedError
from course_supporter.llm.schemas import LLMResponse
from course_supporter.models.course import (
    CourseContext,
    CourseStructure,
    ModuleOutput,
)
from course_supporter.models.source import SourceDocument


@pytest.fixture()
def mock_router() -> AsyncMock:
    """ModelRouter mock with complete_structured returning CourseStructure."""
    router = AsyncMock()
    structure = CourseStructure(
        title="Test Course",
        description="A test course",
        modules=[ModuleOutput(title="Module 1")],
    )
    response = LLMResponse(
        content="{}",
        provider="gemini",
        model_id="gemini-2.5-flash",
        tokens_in=100,
        tokens_out=200,
    )
    router.complete_structured.return_value = (structure, response)
    return router


@pytest.fixture()
def sample_context() -> CourseContext:
    """Minimal CourseContext for testing."""
    doc = SourceDocument(source_type="text", source_url="file:///test.md")
    return CourseContext(documents=[doc])


@pytest.fixture()
def prompt_data() -> dict[str, Any]:
    """Mock prompt data."""
    return {
        "version": "v1",
        "system_prompt": "You are a course architect.",
        "user_prompt_template": "Materials:\n{context}\nGenerate.",
    }


class TestArchitectAgentInit:
    def test_default_params(self, mock_router: AsyncMock) -> None:
        """ArchitectAgent initializes with sensible defaults."""
        agent = ArchitectAgent(mock_router)
        assert agent._prompt_path == DEFAULT_PROMPT_PATH
        assert agent._strategy == "default"
        assert agent._temperature == 0.0
        assert agent._max_tokens == 8192

    def test_custom_params(self, mock_router: AsyncMock) -> None:
        """ArchitectAgent accepts custom parameters."""
        agent = ArchitectAgent(
            mock_router,
            prompt_path="custom/prompt.yaml",
            strategy="quality",
            temperature=0.3,
            max_tokens=4096,
        )
        assert agent._prompt_path == "custom/prompt.yaml"
        assert agent._strategy == "quality"
        assert agent._temperature == 0.3
        assert agent._max_tokens == 4096


class TestPreparePrompts:
    """Tests for _prepare_prompts step (independent of LLM)."""

    def test_returns_prepared_prompt(
        self,
        mock_router: AsyncMock,
        sample_context: CourseContext,
        prompt_data: dict[str, Any],
    ) -> None:
        """_prepare_prompts returns PreparedPrompt with correct fields."""
        with patch(
            "course_supporter.agents.architect.load_prompt",
            return_value=prompt_data,
        ):
            agent = ArchitectAgent(mock_router)
            prepared = agent._prepare_prompts(sample_context)

        assert isinstance(prepared, PreparedPrompt)
        assert prepared.system_prompt == "You are a course architect."
        assert prepared.prompt_version == "v1"

    def test_serializes_context_into_user_prompt(
        self,
        mock_router: AsyncMock,
        sample_context: CourseContext,
        prompt_data: dict[str, Any],
    ) -> None:
        """_prepare_prompts injects serialized context into user prompt."""
        with patch(
            "course_supporter.agents.architect.load_prompt",
            return_value=prompt_data,
        ):
            agent = ArchitectAgent(mock_router)
            prepared = agent._prepare_prompts(sample_context)

        assert "Materials:" in prepared.user_prompt
        assert "file:///test.md" in prepared.user_prompt
        assert "{context}" not in prepared.user_prompt

    def test_propagates_file_not_found(
        self,
        mock_router: AsyncMock,
        sample_context: CourseContext,
    ) -> None:
        """_prepare_prompts propagates FileNotFoundError."""
        agent = ArchitectAgent(
            mock_router, prompt_path="nonexistent/prompt.yaml"
        )
        with pytest.raises(FileNotFoundError):
            agent._prepare_prompts(sample_context)

    def test_default_version_when_missing(
        self,
        mock_router: AsyncMock,
        sample_context: CourseContext,
    ) -> None:
        """_prepare_prompts uses 'unknown' when version key is absent."""
        prompt_data_no_version = {
            "system_prompt": "System prompt.",
            "user_prompt_template": "{context}",
        }
        with patch(
            "course_supporter.agents.architect.load_prompt",
            return_value=prompt_data_no_version,
        ):
            agent = ArchitectAgent(mock_router)
            prepared = agent._prepare_prompts(sample_context)

        assert prepared.prompt_version == "unknown"


class TestGenerate:
    """Tests for _generate step (LLM interaction)."""

    @pytest.mark.asyncio
    async def test_returns_course_structure(
        self, mock_router: AsyncMock
    ) -> None:
        """_generate returns CourseStructure from router response."""
        agent = ArchitectAgent(mock_router)
        prepared = PreparedPrompt(
            system_prompt="System.",
            user_prompt="User prompt.",
            prompt_version="v1",
        )
        result = await agent._generate(prepared)

        assert isinstance(result, CourseStructure)
        assert result.title == "Test Course"
        assert len(result.modules) == 1

    @pytest.mark.asyncio
    async def test_calls_router_with_correct_params(
        self, mock_router: AsyncMock
    ) -> None:
        """_generate passes correct params to router."""
        agent = ArchitectAgent(
            mock_router, strategy="quality", temperature=0.5, max_tokens=2048
        )
        prepared = PreparedPrompt(
            system_prompt="System prompt.",
            user_prompt="User prompt.",
            prompt_version="v1",
        )
        await agent._generate(prepared)

        mock_router.complete_structured.assert_called_once()
        call_kwargs = mock_router.complete_structured.call_args.kwargs
        assert call_kwargs["action"] == "course_structuring"
        assert call_kwargs["response_schema"] is CourseStructure
        assert call_kwargs["system_prompt"] == "System prompt."
        assert call_kwargs["prompt"] == "User prompt."
        assert call_kwargs["strategy"] == "quality"
        assert call_kwargs["temperature"] == 0.5
        assert call_kwargs["max_tokens"] == 2048

    @pytest.mark.asyncio
    async def test_propagates_all_models_failed(
        self, mock_router: AsyncMock
    ) -> None:
        """_generate propagates AllModelsFailedError from router."""
        mock_router.complete_structured.side_effect = AllModelsFailedError(
            action="course_structuring",
            strategies_tried=["default"],
            errors=[("gemini-2.5-flash", "rate limit")],
        )
        agent = ArchitectAgent(mock_router)
        prepared = PreparedPrompt(
            system_prompt="System.",
            user_prompt="User.",
            prompt_version="v1",
        )
        with pytest.raises(AllModelsFailedError):
            await agent._generate(prepared)


class TestArchitectAgentRun:
    """Integration tests for full run() pipeline."""

    @pytest.mark.asyncio
    async def test_run_end_to_end(
        self,
        mock_router: AsyncMock,
        sample_context: CourseContext,
        prompt_data: dict[str, Any],
    ) -> None:
        """run() orchestrates prepare + generate and returns structure."""
        with patch(
            "course_supporter.agents.architect.load_prompt",
            return_value=prompt_data,
        ):
            agent = ArchitectAgent(mock_router)
            result = await agent.run(sample_context)

        assert isinstance(result, CourseStructure)
        assert result.title == "Test Course"
        assert len(result.modules) == 1

    @pytest.mark.asyncio
    async def test_run_passes_context_to_router(
        self,
        mock_router: AsyncMock,
        sample_context: CourseContext,
        prompt_data: dict[str, Any],
    ) -> None:
        """run() serializes context and passes it through to router."""
        with patch(
            "course_supporter.agents.architect.load_prompt",
            return_value=prompt_data,
        ):
            agent = ArchitectAgent(mock_router)
            await agent.run(sample_context)

        call_kwargs = mock_router.complete_structured.call_args.kwargs
        assert "file:///test.md" in call_kwargs["prompt"]
        assert call_kwargs["system_prompt"] == "You are a course architect."
```

---

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª—ñ–≤

```
src/course_supporter/agents/
‚îú‚îÄ‚îÄ __init__.py                  # UPDATE: add ArchitectAgent, PreparedPrompt exports
‚îú‚îÄ‚îÄ architect.py                 # UPDATE: replace stub with step-based implementation
‚îî‚îÄ‚îÄ prompt_loader.py             # FROM S1-020

tests/unit/
‚îî‚îÄ‚îÄ test_architect_agent.py      # NEW: ~12 tests (per-step + integration)
```

---

## –ö—Ä–æ–∫–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è

1. –ó–∞–º—ñ–Ω–∏—Ç–∏ stub `agents/architect.py` ‚Äî `PreparedPrompt` + step-based `ArchitectAgent`
2. –û–Ω–æ–≤–∏—Ç–∏ `agents/__init__.py` ‚Äî –¥–æ–¥–∞—Ç–∏ `ArchitectAgent`, `PreparedPrompt` exports
3. –°—Ç–≤–æ—Ä–∏—Ç–∏ `tests/unit/test_architect_agent.py`
4. `make check`

---

## –ü—Ä–∏–º—ñ—Ç–∫–∏

- **action="course_structuring"**: –≤–∂–µ –≤–∏–∑–Ω–∞—á–µ–Ω–∏–π —É `config/models.yaml` –∑ `requires: [structured_output]`. Default chain: `gemini-2.5-flash ‚Üí deepseek-chat`. Quality: `claude-sonnet ‚Üí gemini-2.5-pro`.
- **complete_structured()** –ø–æ–≤–µ—Ä—Ç–∞—î `tuple[Any, LLMResponse]`. –ü–µ—Ä—à–∏–π –µ–ª–µ–º–µ–Ω—Ç ‚Äî parsed Pydantic model (CourseStructure). ModelRouter –æ–±—Ä–æ–±–ª—è—î retry —ñ fallback.
- **PreparedPrompt**: `NamedTuple` (–Ω–µ dataclass) ‚Äî immutable, lightweight, unpacking-friendly. –°—Ç–∞–Ω–µ —á–∞—Å—Ç–∏–Ω–æ—é `GraphState` –ø—Ä–∏ –º—ñ–≥—Ä–∞—Ü—ñ—ó –Ω–∞ LangGraph.
- **Step isolation**: `_prepare_prompts` ‚Äî sync (CPU-only), `_generate` ‚Äî async (I/O). –†—ñ–∑–Ω–∞ –ø—Ä–∏—Ä–æ–¥–∞ –¥–æ–∑–≤–æ–ª—è—î —Ä—ñ–∑–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è.
- **–°–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è context**: `context.model_dump_json(indent=2)` ‚Äî JSON representation. LLM –æ—Ç—Ä–∏–º—É—î –ø–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —è–∫ —Ç–µ–∫—Å—Ç.
- **max_tokens=8192**: CourseStructure –º–æ–∂–µ –±—É—Ç–∏ –≤–µ–ª–∏–∫–æ—é (–¥–µ—Å—è—Ç–∫–∏ –º–æ–¥—É–ª—ñ–≤, —Å–æ—Ç–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ–π). 8192 ‚Äî —Ä–æ–∑—É–º–Ω–∏–π default.
- **Structured output validation**: Pydantic validation –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ provider.complete_structured(). –Ø–∫—â–æ LLM –ø–æ–≤–µ—Ä—Ç–∞—î –Ω–µ–≤–∞–ª—ñ–¥–Ω–∏–π JSON ‚Äî `StructuredOutputError`, router retry/fallback.
- **–ù–µ –≥–ª—É—à–∏–º–æ –ø–æ–º–∏–ª–∫–∏**: `AllModelsFailedError` –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è –¥–æ caller (API endpoint –∞–±–æ orchestrator).

## –®–ª—è—Ö –º—ñ–≥—Ä–∞—Ü—ñ—ó –Ω–∞ –≥—Ä–∞—Ñ–∏/–ª–∞–Ω—Ü—é–≥–∏

| –°—Ü–µ–Ω–∞—Ä—ñ–π | –©–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ | Effort |
|----------|-------------|--------|
| **–î–æ–¥–∞—Ç–∏ validation step** | –ù–æ–≤–∏–π –º–µ—Ç–æ–¥ `_validate(structure) -> CourseStructure`, –≤–∏–∫–ª–∏–∫ –º—ñ–∂ `_generate` —ñ `return` | ~1 –≥–æ–¥ |
| **–î–æ–¥–∞—Ç–∏ refinement loop** | `_refine(structure, feedback) -> CourseStructure`, loop —É `run()` –∞–±–æ edge —É –≥—Ä–∞—Ñ—ñ | ~2 –≥–æ–¥ |
| **Multi-step generation** | –†–æ–∑–±–∏—Ç–∏ `_generate` –Ω–∞ `_generate_modules()` + `_generate_lessons()` + `_generate_exercises()` | ~4 –≥–æ–¥ |
| **LangGraph –º—ñ–≥—Ä–∞—Ü—ñ—è** | –ö–æ–∂–µ–Ω `_method` —Å—Ç–∞—î `@node`, `PreparedPrompt` –≤—Ö–æ–¥–∏—Ç—å —É `GraphState`, edges = conditional routing | ~8 –≥–æ–¥ |
| **Custom DAG** | –û–±–≥–æ—Ä—Ç–∫–∞ `Pipeline(nodes=[prepare, generate, validate])` –∑ logging per-node | ~4 –≥–æ–¥ |
