# üìã S1-016: WebProcessor (URL ‚Üí trafilatura)

## –ú–µ—Ç–∞

–†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –æ–±—Ä–æ–±–∫—É –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–æ–∫: fetch HTML —á–µ—Ä–µ–∑ `trafilatura`, –≤–∏—Ç—è–≥—Ç–∏ –æ—Å–Ω–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∑–±–µ—Ä–µ–≥—Ç–∏ raw HTML —è–∫ content snapshot –¥–ª—è –º–æ–∂–ª–∏–≤–æ—ó –ø–µ—Ä–µ–æ–±—Ä–æ–±–∫–∏. –ë–µ–∑ LLM ‚Äî pure extraction.

## –ö–æ–Ω—Ç–µ–∫—Å—Ç

–ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ S1-011 (schemas + ABC). –ù–µ –ø–æ—Ç—Ä–µ–±—É—î `ModelRouter`. –ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ `trafilatura` –≤–∂–µ –≤ `pyproject.toml`. `trafilatura` ‚Äî specialized lib –¥–ª—è web content extraction (–∫—Ä–∞—â–µ –∑–∞ BS4 –¥–ª—è —Å—Ç–∞—Ç–µ–π/–±–ª–æ–≥—ñ–≤).

---

## Acceptance Criteria

- [ ] `WebProcessor` —Ä–µ–∞–ª—ñ–∑—É—î `SourceProcessor.process()`
- [ ] `trafilatura.fetch_url()` –æ—Ç—Ä–∏–º—É—î HTML
- [ ] `trafilatura.extract()` –≤–∏—Ç—è–≥—É—î –æ—Å–Ω–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
- [ ] Raw HTML –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –≤ `metadata["content_snapshot"]`
- [ ] Domain –≤–∏—Ç—è–≥–Ω—É—Ç–æ –≤ `metadata["domain"]`
- [ ] `fetched_at` timestamp —É metadata
- [ ] Fetch failure (returns None) ‚Üí `ProcessingError`
- [ ] Extract returns None ‚Üí empty chunks (–Ω–µ error)
- [ ] Non-web source_type ‚Üí `UnsupportedFormatError`
- [ ] ~7 unit-—Ç–µ—Å—Ç—ñ–≤
- [ ] `make check` –ø—Ä–æ—Ö–æ–¥–∏—Ç—å

---

## –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

### src/course_supporter/ingestion/web.py

```python
"""Web processor using trafilatura for content extraction."""

from datetime import datetime
from urllib.parse import urlparse

import structlog

from course_supporter.ingestion.base import (
    ProcessingError,
    SourceProcessor,
    UnsupportedFormatError,
)
from course_supporter.models.source import (
    ChunkType,
    ContentChunk,
    SourceDocument,
)

logger = structlog.get_logger()


class WebProcessor(SourceProcessor):
    """Process web pages by fetching HTML and extracting content.

    Uses trafilatura for intelligent content extraction
    (article text, removing boilerplate/navigation).
    Raw HTML is saved as content_snapshot for re-processing.
    """

    async def process(
        self,
        source: "SourceMaterial",
        *,
        router: "ModelRouter | None" = None,
    ) -> SourceDocument:
        if source.source_type != "web":
            raise UnsupportedFormatError(
                f"WebProcessor expects 'web', got '{source.source_type}'"
            )

        url = source.source_url
        parsed_url = urlparse(url)
        domain = parsed_url.netloc

        logger.info("web_processing_start", url=url, domain=domain)

        # 1. Fetch HTML
        html = await self._fetch_html(url)

        # 2. Extract content
        extracted = self._extract_content(html)

        # 3. Split into chunks
        chunks = self._text_to_chunks(extracted) if extracted else []

        fetched_at = datetime.now().isoformat()

        logger.info(
            "web_processing_done",
            url=url,
            chunk_count=len(chunks),
        )

        return SourceDocument(
            source_type="web",
            source_url=url,
            title=source.filename or domain,
            chunks=chunks,
            metadata={
                "domain": domain,
                "fetched_at": fetched_at,
                "content_snapshot": html,
            },
        )

    @staticmethod
    async def _fetch_html(url: str) -> str:
        """Fetch HTML from URL using trafilatura.

        Args:
            url: The URL to fetch.

        Returns:
            Raw HTML string.

        Raises:
            ProcessingError: If fetch fails (returns None).
        """
        import trafilatura  # type: ignore[import-untyped]

        html = trafilatura.fetch_url(url)
        if html is None:
            raise ProcessingError(
                f"Failed to fetch URL: {url}. "
                "The page may be unreachable or blocked."
            )
        return html

    @staticmethod
    def _extract_content(html: str) -> str | None:
        """Extract main content from HTML using trafilatura.

        Args:
            html: Raw HTML string.

        Returns:
            Extracted text content, or None if extraction fails.
        """
        import trafilatura  # type: ignore[import-untyped]

        return trafilatura.extract(
            html,
            include_comments=False,
            include_tables=True,
        )

    @staticmethod
    def _text_to_chunks(text: str) -> list[ContentChunk]:
        """Split extracted text into content chunks.

        Splits on double newlines to create paragraph-like chunks.
        """
        chunks: list[ContentChunk] = []
        paragraphs = text.strip().split("\n\n")

        for idx, para in enumerate(paragraphs):
            para = para.strip()
            if not para:
                continue
            chunks.append(
                ContentChunk(
                    chunk_type=ChunkType.WEB_CONTENT,
                    text=para,
                    index=idx,
                )
            )

        return chunks
```

---

## –¢–µ—Å—Ç–∏

### tests/unit/test_ingestion/test_web.py

```python
"""Tests for WebProcessor."""

from unittest.mock import MagicMock, patch

import pytest

from course_supporter.ingestion.base import ProcessingError, UnsupportedFormatError
from course_supporter.ingestion.web import WebProcessor
from course_supporter.models.source import ChunkType, SourceDocument


def _make_source(
    source_type: str = "web",
    url: str = "https://example.com/article",
    filename: str | None = None,
) -> MagicMock:
    source = MagicMock()
    source.source_type = source_type
    source.source_url = url
    source.filename = filename
    return source


class TestWebProcessor:
    async def test_web_fetch_success(self) -> None:
        """Mock trafilatura ‚Üí SourceDocument with WEB_CONTENT chunks."""
        with (
            patch("trafilatura.fetch_url", return_value="<html>content</html>"),
            patch("trafilatura.extract", return_value="Extracted paragraph one\n\nParagraph two"),
        ):
            proc = WebProcessor()
            doc = await proc.process(_make_source())

        assert isinstance(doc, SourceDocument)
        assert doc.source_type == "web"
        assert len(doc.chunks) == 2
        assert doc.chunks[0].chunk_type == ChunkType.WEB_CONTENT

    async def test_web_fetch_failure(self) -> None:
        """fetch_url returns None ‚Üí ProcessingError."""
        with patch("trafilatura.fetch_url", return_value=None):
            proc = WebProcessor()
            with pytest.raises(ProcessingError, match="Failed to fetch URL"):
                await proc.process(_make_source())

    async def test_web_extract_empty(self) -> None:
        """extract returns None ‚Üí empty chunks (not an error)."""
        with (
            patch("trafilatura.fetch_url", return_value="<html></html>"),
            patch("trafilatura.extract", return_value=None),
        ):
            proc = WebProcessor()
            doc = await proc.process(_make_source())

        assert doc.chunks == []

    async def test_web_domain_in_metadata(self) -> None:
        """URL domain extracted to metadata."""
        with (
            patch("trafilatura.fetch_url", return_value="<html>ok</html>"),
            patch("trafilatura.extract", return_value="text"),
        ):
            proc = WebProcessor()
            doc = await proc.process(
                _make_source(url="https://docs.python.org/3/tutorial.html")
            )

        assert doc.metadata["domain"] == "docs.python.org"

    async def test_web_invalid_source_type(self) -> None:
        """Non-web source_type ‚Üí UnsupportedFormatError."""
        proc = WebProcessor()
        with pytest.raises(UnsupportedFormatError, match="expects 'web'"):
            await proc.process(_make_source(source_type="text"))

    async def test_web_content_snapshot(self) -> None:
        """Raw HTML saved in metadata for later re-processing."""
        raw_html = "<html><body>Raw content</body></html>"
        with (
            patch("trafilatura.fetch_url", return_value=raw_html),
            patch("trafilatura.extract", return_value="Content"),
        ):
            proc = WebProcessor()
            doc = await proc.process(_make_source())

        assert doc.metadata["content_snapshot"] == raw_html

    async def test_web_chunks_indexed(self) -> None:
        """Multiple paragraphs ‚Üí ordered chunks."""
        text = "Para 1\n\nPara 2\n\nPara 3"
        with (
            patch("trafilatura.fetch_url", return_value="<html>ok</html>"),
            patch("trafilatura.extract", return_value=text),
        ):
            proc = WebProcessor()
            doc = await proc.process(_make_source())

        assert len(doc.chunks) == 3
        indices = [c.index for c in doc.chunks]
        assert indices == [0, 1, 2]
```

---

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª—ñ–≤

```
src/course_supporter/ingestion/
‚îú‚îÄ‚îÄ web.py                   # WebProcessor

tests/unit/test_ingestion/
‚îî‚îÄ‚îÄ test_web.py              # ~7 tests
```

---

## –ö—Ä–æ–∫–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è

1. –ü–µ—Ä–µ–∫–æ–Ω–∞—Ç–∏—Å—è, —â–æ S1-011 –∑–∞–≤–µ—Ä—à–µ–Ω–æ
2. –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `WebProcessor` –≤ `ingestion/web.py`
3. –°—Ç–≤–æ—Ä–∏—Ç–∏ `tests/unit/test_ingestion/test_web.py`
4. `make check`

---

## –ü—Ä–∏–º—ñ—Ç–∫–∏

- **trafilatura vs BeautifulSoup**: `trafilatura` —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è –Ω–∞ extraction –≥–æ–ª–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É (—Å—Ç–∞—Ç—Ç—ñ, –±–ª–æ–≥–∏), –≤–∏–¥–∞–ª—è—é—á–∏ navigation, sidebar, footer. BS4 ‚Äî generic HTML parser. –î–ª—è web scraping trafilatura –∫—Ä–∞—â–µ.
- **Content snapshot**: raw HTML –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –≤ `metadata["content_snapshot"]` –¥–ª—è –º–æ–∂–ª–∏–≤–æ—ó –ø–µ—Ä–µ–æ–±—Ä–æ–±–∫–∏ –∑ —ñ–Ω—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∞–±–æ LLM. –£ production —Ü–µ –±—É–¥–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏—Å—è –≤ `SourceMaterial.content_snapshot` (ORM field).
- **Sync trafilatura –≤ async**: `trafilatura.fetch_url()` ‚Äî sync HTTP call. –î–ª—è MVP –ø—Ä–∏–π–Ω—è—Ç–Ω–æ. –ü—Ä–∏ –ø–æ—Ç—Ä–µ–±—ñ ‚Äî –æ–±–µ—Ä–Ω—É—Ç–∏ –≤ `asyncio.to_thread()`.
- **Domain extraction**: `urlparse(url).netloc` ‚Äî –ø—Ä–æ—Å—Ç–∏–π —ñ –Ω–∞–¥—ñ–π–Ω–∏–π —Å–ø–æ—Å—ñ–± –æ—Ç—Ä–∏–º–∞—Ç–∏ domain.
- **Empty extract ‚â† error**: —è–∫—â–æ trafilatura –Ω–µ –º–æ–∂–µ –≤–∏—Ç—è–≥—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç (—Å—Ç–æ—Ä—ñ–Ω–∫–∞ –∑ JS-only), –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ empty chunks, –Ω–µ ProcessingError. Fetch failure (network error) ‚Äî —Ü–µ ProcessingError.
