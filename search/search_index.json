{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Course Supporter","text":"<p>AI-powered system for transforming course materials into structured learning plans with automated mentoring.</p>"},{"location":"#what-it-does","title":"What it does","text":"<ul> <li>Ingests video, presentations, text, and web links</li> <li>Processes content via LLM-powered pipeline (Gemini, Anthropic, OpenAI, DeepSeek)</li> <li>Generates structured course outlines with modules, lessons, concepts, and exercises</li> <li>Serves results via multi-tenant REST API with API key authentication</li> </ul>"},{"location":"#tech-stack","title":"Tech Stack","text":"Layer Technology Language Python 3.13+ API FastAPI Database PostgreSQL 17 + pgvector ORM SQLAlchemy (async) Object Storage S3-compatible (Backblaze B2) LLM Providers Gemini, Anthropic, OpenAI, DeepSeek Containerization Docker Compose CI/CD GitHub Actions"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Architecture &amp; ERD</li> <li>API Flow Guide</li> <li>Development Setup</li> <li>Sprint Roadmap</li> </ul>"},{"location":"api/auth/","title":"Authentication &amp; Authorization","text":"<p>Coming in Sprint 2, Epic 7</p> <p>Detailed auth guide with examples will be published here.</p> <p>Current auth model (since Sprint 1):</p> <ul> <li>API key via <code>X-API-Key</code> header</li> <li>Scopes: <code>prep</code> (course preparation), <code>check</code> (homework checking)</li> <li>Rate limiting per tenant + scope</li> <li>Keys managed via Admin CLI (<code>manage_tenant.py</code>)</li> </ul> <p>For architecture details, see ADR-008: API key auth.</p>"},{"location":"api/flow-guide/","title":"API Flow Guide","text":"<p>Coming in Sprint 2, Epic 7</p> <p>Step-by-step guide from course creation to structure generation will be published here.</p> <p>For the architecture overview, see Sprint 2 plan.</p>"},{"location":"api/reference/","title":"API Reference","text":"<p>Coming in Sprint 2, Epic 7</p> <p>OpenAPI-based reference with request/response examples will be published here after the API stabilizes.</p> <p>For the current target API design, see Sprint 2 \u2014 Target API.</p>"},{"location":"architecture/decisions/","title":"Architecture Decision Records","text":"<p>Key architectural decisions made throughout the project, organized by sprint.</p>"},{"location":"architecture/decisions/#sprint-0-materials-to-structure-mvp","title":"Sprint 0 \u2014 Materials-to-Structure MVP","text":""},{"location":"architecture/decisions/#adr-001-psycopg-v3-only","title":"ADR-001: psycopg v3 only","text":"<p>Decision: Use <code>postgresql+psycopg://</code> (psycopg v3) as the only DB driver.</p> <p>Context: psycopg v3 supports both sync and async modes natively. Alembic uses sync template, application uses async \u2014 same driver handles both.</p> <p>Consequences: No psycopg2 dependency. Alembic sync migrations work with the same connection string.</p>"},{"location":"architecture/decisions/#adr-002-uuidv7-for-all-primary-keys","title":"ADR-002: UUIDv7 for all primary keys","text":"<p>Decision: Use UUIDv7 (via <code>uuid-utils</code>) for all table PKs.</p> <p>Context: Time-ordered UUIDs are sortable, avoid sequence bottlenecks, and work well in distributed systems.</p> <p>Consequences: No auto-increment sequences. IDs are sortable by creation time. Slightly larger than integer PKs.</p>"},{"location":"architecture/decisions/#adr-003-pep-735-dependency-groups","title":"ADR-003: PEP 735 dependency groups","text":"<p>Decision: Use <code>[dependency-groups]</code> (PEP 735) for dev and docs tooling. Use <code>[project.optional-dependencies]</code> only for <code>media</code> (Whisper + PyTorch ~2GB).</p> <p>Context: PEP 735 cleanly separates development tools from runtime dependencies. <code>uv sync</code> includes dev by default; <code>uv sync --group docs</code> adds docs tools.</p> <p>Consequences: <code>media</code> stays as optional dependency because it affects the Docker image size and is needed at runtime.</p>"},{"location":"architecture/decisions/#adr-004-strategy-based-modelrouter-with-two-level-fallback","title":"ADR-004: Strategy-based ModelRouter with two-level fallback","text":"<p>Decision: <code>ModelRouter</code> selects LLM provider based on action + strategy from <code>config/models.yaml</code>. Two-level fallback: within chain (next model) + cross-strategy (fallback strategy).</p> <p>Context: 4 LLM providers with different strengths. Need graceful degradation when a provider is down.</p> <p>Consequences: Any component calls LLM through <code>ModelRouter</code> without knowing provider details. Error classification (permanent vs transient) determines retry behavior.</p>"},{"location":"architecture/decisions/#adr-005-composition-pattern-for-videoprocessor","title":"ADR-005: Composition pattern for VideoProcessor","text":"<p>Decision: <code>VideoProcessor</code> is a shell that composes <code>GeminiVideoProcessor</code> (primary) and <code>WhisperVideoProcessor</code> (fallback) as separate classes.</p> <p>Context: Gemini handles short videos directly; Whisper is needed for long videos. Different APIs, different resource requirements.</p> <p>Consequences: Each processor is independently testable. Fallback logic is in the shell, not mixed with processing.</p>"},{"location":"architecture/decisions/#adr-006-repository-flush-not-commit","title":"ADR-006: Repository flush() not commit()","text":"<p>Decision: Repositories call <code>flush()</code> instead of <code>commit()</code>. The caller controls transaction boundaries.</p> <p>Context: Multiple repository operations often need to be atomic. If repositories committed, partial failures would leave inconsistent data.</p> <p>Consequences: Service layer or endpoint handler calls <code>session.commit()</code> after all operations succeed.</p>"},{"location":"architecture/decisions/#adr-007-selectinload-chains-not-joinedload","title":"ADR-007: selectinload chains (not joinedload)","text":"<p>Decision: Use <code>selectinload</code> for eager loading of relationships, not <code>joinedload</code>.</p> <p>Context: <code>joinedload</code> with multiple one-to-many relationships creates cartesian products, returning N\u00d7M rows.</p> <p>Consequences: N+1 queries become N+1 SELECTs (batched), but no cartesian explosion. Acceptable for typical depths.</p>"},{"location":"architecture/decisions/#sprint-1-production-deploy","title":"Sprint 1 \u2014 Production Deploy","text":""},{"location":"architecture/decisions/#adr-008-api-key-auth-not-oauthjwt","title":"ADR-008: API key auth (not OAuth/JWT)","text":"<p>Decision: Simple API key authentication via <code>X-API-Key</code> header. Key stored as SHA-256 hash.</p> <p>Context: B2B API with a small number of tenants. OAuth/JWT adds complexity without proportional benefit.</p> <p>Consequences: Stateless auth. Easy key rotation via Admin CLI. Raw key never stored in DB.</p>"},{"location":"architecture/decisions/#adr-009-in-memory-rate-limiter","title":"ADR-009: In-memory rate limiter","text":"<p>Decision: Sliding window rate limiter in application memory, per (tenant_id, scope).</p> <p>Context: Single-process deployment on one VPS. Redis-based limiter would add infrastructure for no gain.</p> <p>Consequences: Rate limits reset on app restart. Clear upgrade path to Redis when horizontal scaling is needed.</p>"},{"location":"architecture/decisions/#adr-010-shared-docker-network-for-nginx","title":"ADR-010: Shared Docker network for nginx","text":"<p>Decision: App container connects to a shared Docker network where nginx (from a separate compose project) runs. No ports exposed to host.</p> <p>Context: VPS runs multiple services behind one nginx. Each service is a separate Docker Compose project.</p> <p>Consequences: All traffic goes through nginx. Security headers, TLS termination, and rate limiting at nginx level. <code>resolver 127.0.0.11</code> pattern for dynamic DNS.</p>"},{"location":"architecture/decisions/#adr-011-s3-multipart-upload-10mb-parts","title":"ADR-011: S3 multipart upload (10MB parts)","text":"<p>Decision: Files &gt; 50MB use multipart upload with 10MB parts via aiobotocore. Streaming from client through nginx to S3.</p> <p>Context: Course materials can be up to 1GB (video files). Must not buffer entire file in memory.</p> <p>Consequences: Constant ~10-20 MB RAM regardless of file size. <code>proxy_request_buffering off</code> in nginx config.</p>"},{"location":"architecture/decisions/#sprint-2-material-tree-in-progress","title":"Sprint 2 \u2014 Material Tree (in progress)","text":"<p>See Sprint 2 plan for AR-1 through AR-7.</p>"},{"location":"architecture/erd/","title":"Entity Relationship Diagram","text":"<p>Updated per sprint</p> <p>This ERD reflects the current database schema and is updated with each sprint. Source: <code>current-doc/Sprint-2-ERD.mermaid</code></p> <pre><code>erDiagram\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    %% Multi-Tenant Auth\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    Tenant {\n        uuid id PK \"UUIDv7\"\n        string name UK \"unique\"\n        bool is_active\n        timestamptz created_at\n        timestamptz updated_at\n    }\n\n    APIKey {\n        uuid id PK \"UUIDv7\"\n        uuid tenant_id FK\n        string key_hash UK \"sha256, indexed\"\n        string key_prefix\n        string label\n        jsonb scopes \"['prep','check']\"\n        int rate_limit_prep \"default 60\"\n        int rate_limit_check \"default 300\"\n        bool is_active\n        timestamptz expires_at \"nullable\"\n        timestamptz created_at\n    }\n\n    Tenant ||--o{ APIKey : \"has keys\"\n\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    %% Course (tenant-scoped root)\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    Course {\n        uuid id PK \"UUIDv7\"\n        uuid tenant_id FK \"indexed\"\n        string title\n        text description \"nullable\"\n        text learning_goal \"nullable\"\n        jsonb expected_knowledge \"nullable\"\n        jsonb expected_skills \"nullable\"\n        timestamptz created_at\n        timestamptz updated_at\n    }\n\n    Tenant ||--o{ Course : \"owns\"\n\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    %% Material Tree (recursive adjacency list)\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    MaterialNode {\n        uuid id PK \"UUIDv7\"\n        uuid course_id FK \"indexed\"\n        uuid parent_id FK \"self-ref, nullable, indexed\"\n        string title\n        text description \"nullable\"\n        int order \"default 0\"\n        string node_fingerprint \"lazy cached Merkle hash, nullable\"\n        timestamptz created_at\n        timestamptz updated_at\n    }\n\n    Course ||--o{ MaterialNode : \"has tree\"\n    MaterialNode ||--o{ MaterialNode : \"children (parent_id)\"\n\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    %% Material Entry (raw + processed + state)\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    MaterialEntry {\n        uuid id PK \"UUIDv7\"\n        uuid node_id FK \"indexed\"\n        enum source_type \"video|presentation|text|web\"\n        int order \"default 0\"\n        string source_url \"S3 or external URL\"\n        string filename \"nullable\"\n        string raw_hash \"lazy cached sha256, nullable\"\n        int raw_size_bytes \"nullable\"\n        string processed_hash \"raw_hash at processing time, nullable\"\n        text processed_content \"SourceDocument JSON, nullable\"\n        timestamptz processed_at \"nullable\"\n        uuid pending_job_id FK \"nullable, receipt\"\n        timestamptz pending_since \"nullable\"\n        string content_fingerprint \"lazy cached sha256, nullable\"\n        text error_message \"nullable\"\n        timestamptz created_at\n        timestamptz updated_at\n    }\n\n    MaterialNode ||--o{ MaterialEntry : \"has materials\"\n\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    %% Slide-Video Mapping\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    SlideVideoMapping {\n        uuid id PK \"UUIDv7\"\n        uuid node_id FK \"indexed\"\n        uuid presentation_entry_id FK \"MaterialEntry (presentation)\"\n        uuid video_entry_id FK \"MaterialEntry (video)\"\n        int slide_number \"slide in this presentation\"\n        string video_timecode_start \"HH:MM:SS\"\n        string video_timecode_end \"nullable, HH:MM:SS\"\n        int order \"appearance order in video\"\n        string validation_state \"validated|pending_validation|validation_failed\"\n        jsonb blocking_factors \"nullable, what blocks validation\"\n        jsonb validation_errors \"nullable, failed checks\"\n        timestamptz validated_at \"nullable\"\n        timestamptz created_at\n    }\n\n    MaterialNode ||--o{ SlideVideoMapping : \"has mappings\"\n    MaterialEntry ||--o{ SlideVideoMapping : \"as presentation\"\n    MaterialEntry ||--o{ SlideVideoMapping : \"as video\"\n\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    %% Job Tracking (ARQ + persistence)\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    Job {\n        uuid id PK \"UUIDv7\"\n        uuid course_id FK \"nullable, indexed\"\n        uuid node_id FK \"nullable, indexed\"\n        string job_type \"ingestion|structure_generation\"\n        string priority \"normal|immediate\"\n        string status \"queued|active|complete|failed\"\n        string arq_job_id \"ARQ internal ID\"\n        jsonb input_params\n        uuid result_material_id FK \"nullable, CHECK: at most one result FK\"\n        uuid result_snapshot_id FK \"nullable, CHECK: at most one result FK\"\n        jsonb depends_on \"nullable, list of job_ids\"\n        text error_message \"nullable\"\n        timestamptz queued_at\n        timestamptz started_at \"nullable\"\n        timestamptz completed_at \"nullable\"\n        timestamptz estimated_at \"nullable\"\n    }\n\n    Course ||--o{ Job : \"has jobs\"\n    MaterialNode ||--o{ Job : \"scoped to node\"\n    MaterialEntry }o--o| Job : \"pending_job_id\"\n    Job }o--o| MaterialEntry : \"result_material_id\"\n    Job }o--o| CourseStructureSnapshot : \"result_snapshot_id\"\n\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    %% Course Structure Snapshots\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    CourseStructureSnapshot {\n        uuid id PK \"UUIDv7\"\n        uuid course_id FK \"indexed\"\n        uuid node_id FK \"nullable (NULL=whole course), indexed\"\n        string node_fingerprint \"Merkle hash at generation time\"\n        string mode \"free|guided\"\n        jsonb structure \"CourseStructure JSON\"\n        string prompt_version \"nullable\"\n        string model_id \"nullable\"\n        int tokens_in \"nullable\"\n        int tokens_out \"nullable\"\n        float cost_usd \"nullable\"\n        timestamptz created_at\n    }\n\n    Course ||--o{ CourseStructureSnapshot : \"has snapshots\"\n    MaterialNode ||--o{ CourseStructureSnapshot : \"scoped to node\"\n\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    %% Course Structure (generated output)\n    %% Snapshot stores raw LLM output as JSONB;\n    %% when \"applied\", unpacked into normalized tables below.\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    Module {\n        uuid id PK \"UUIDv7\"\n        uuid course_id FK\n        uuid snapshot_id FK \"nullable, source snapshot\"\n        string title\n        text description \"nullable\"\n        text learning_goal \"nullable\"\n        jsonb expected_knowledge \"nullable\"\n        jsonb expected_skills \"nullable\"\n        string difficulty \"easy|medium|hard\"\n        int order\n        timestamptz created_at\n    }\n\n    Course ||--o{ Module : \"has modules\"\n    CourseStructureSnapshot ||--o{ Module : \"applied from\"\n\n    Lesson {\n        uuid id PK \"UUIDv7\"\n        uuid module_id FK\n        string title\n        int order\n        string video_start_timecode \"nullable\"\n        string video_end_timecode \"nullable\"\n        jsonb slide_range \"nullable\"\n        timestamptz created_at\n    }\n\n    Module ||--o{ Lesson : \"has lessons\"\n\n    Concept {\n        uuid id PK \"UUIDv7\"\n        uuid lesson_id FK\n        string title\n        text definition\n        jsonb examples \"nullable\"\n        jsonb timecodes \"nullable\"\n        jsonb slide_references \"nullable\"\n        jsonb web_references \"nullable\"\n        vector embedding \"1536 dims, nullable\"\n        timestamptz created_at\n    }\n\n    Lesson ||--o{ Concept : \"has concepts\"\n\n    Exercise {\n        uuid id PK \"UUIDv7\"\n        uuid lesson_id FK\n        text description\n        text reference_solution \"nullable\"\n        text grading_criteria \"nullable\"\n        int difficulty_level \"nullable, 1-5\"\n        timestamptz created_at\n    }\n\n    Lesson ||--o{ Exercise : \"has exercises\"\n\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    %% Observability\n    %% \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    LLMCall {\n        uuid id PK \"UUIDv7\"\n        uuid tenant_id FK \"nullable, indexed\"\n        string action\n        string strategy\n        string provider\n        string model_id\n        string prompt_version \"nullable\"\n        int tokens_in \"nullable\"\n        int tokens_out \"nullable\"\n        int latency_ms \"nullable\"\n        float cost_usd \"nullable\"\n        bool success\n        text error_message \"nullable\"\n        timestamptz created_at\n    }\n\n    Tenant ||--o{ LLMCall : \"tracked calls\"\n</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"architecture/infrastructure/","title":"Infrastructure","text":""},{"location":"architecture/infrastructure/#production-environment","title":"Production Environment","text":"Component Details VPS 8 GB RAM, 2 vCPU (Xeon Gold 6132), 32 GB disk OS Ubuntu, Docker Engine Domain <code>api.pythoncourse.me</code> TLS Let's Encrypt (certbot, auto-renewal), TLSv1.2/1.3, HSTS"},{"location":"architecture/infrastructure/#container-architecture","title":"Container Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       shared-net                          \u2502\n\u2502                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502  \u2502  nginx   \u2502\u2500\u2500\u25b6\u2502 course-supporter \u2502                     \u2502\n\u2502  \u2502 (Django  \u2502   \u2502      -app        \u2502                     \u2502\n\u2502  \u2502 compose) \u2502   \u2502  (FastAPI:8000)  \u2502                     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518                     \u2502\n\u2502       \u2502            \u2502           \u2502                          \u2502\n\u2502       \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502       \u2502    \u2502  postgres  \u2502  \u2502       redis         \u2502        \u2502\n\u2502       \u2502    \u2502    -cs     \u2502  \u2502  (redis:7-alpine)   \u2502        \u2502\n\u2502       \u2502    \u2502(pgvector)  \u2502  \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502       \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502                           \u2502\n\u2502       \u2502                  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502       \u2502                  \u2502 course-supporter     \u2502         \u2502\n\u2502       \u2502                  \u2502     -worker          \u2502         \u2502\n\u2502       \u2502                  \u2502  (ARQ consumer)      \u2502         \u2502\n\u2502       \u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502       \u2502                                                   \u2502\n\u2502       \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502    netdata       \u2502                      \u2502\n\u2502                 \u2502  (monitoring)    \u2502                       \u2502\n\u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>nginx lives in a separate Django project compose, connects via <code>shared-net</code></li> <li>App and DB ports are not exposed to host \u2014 all traffic through nginx</li> <li>Worker has no exposed ports \u2014 consumes jobs from Redis queue</li> <li><code>resolver 127.0.0.11 valid=30s</code> pattern for dynamic container DNS resolution</li> </ul>"},{"location":"architecture/infrastructure/#services","title":"Services","text":""},{"location":"architecture/infrastructure/#application-course-supporter-app","title":"Application (<code>course-supporter-app</code>)","text":"<ul> <li>Image: <code>python:3.13-slim</code> (multi-stage build)</li> <li>Workers: 2 uvicorn workers</li> <li>Entry: <code>python -m uvicorn course_supporter.api:app</code></li> </ul>"},{"location":"architecture/infrastructure/#worker-course-supporter-worker","title":"Worker (<code>course-supporter-worker</code>)","text":"<ul> <li>Image: Same as app (shared build)</li> <li>Entry: <code>python -m arq course_supporter.worker.WorkerSettings</code></li> <li>Concurrency: Configured via <code>WORKER_MAX_JOBS</code> (default 2)</li> <li>Purpose: Processes ingestion jobs from Redis queue (video transcription, PDF extraction, etc.)</li> <li>Depends on: PostgreSQL + Redis</li> </ul>"},{"location":"architecture/infrastructure/#database-course-supporter-db","title":"Database (<code>course-supporter-db</code>)","text":"<ul> <li>Image: <code>pgvector/pgvector:pg17</code></li> <li>Volume: <code>pgdata-cs</code> (named volume)</li> <li>Extensions: pgvector (Vector(1536) for embeddings)</li> </ul>"},{"location":"architecture/infrastructure/#redis-course-supporter-redis","title":"Redis (<code>course-supporter-redis</code>)","text":"<ul> <li>Image: <code>redis:7-alpine</code></li> <li>Volume: <code>redis-data</code> (named volume, AOF persistence)</li> <li>Config: <code>maxmemory 128mb</code>, <code>noeviction</code> policy</li> <li>Purpose: ARQ job queue + health check dependency</li> </ul>"},{"location":"architecture/infrastructure/#monitoring-netdata","title":"Monitoring (<code>netdata</code>)","text":"<ul> <li>Dashboard with basic auth</li> <li>Alerts \u2192 Telegram</li> <li>Three monitoring layers: Netdata (system), <code>/health</code> (app), UptimeRobot (external)</li> </ul>"},{"location":"architecture/infrastructure/#object-storage","title":"Object Storage","text":"<ul> <li>Provider: Backblaze B2 (S3-compatible)</li> <li>Bucket: <code>course-supporter</code></li> <li>Endpoint: <code>s3.eu-central-003.backblazeb2.com</code></li> <li>Upload: Multipart (10MB parts) for files &gt; 50MB, constant ~10-20 MB RAM</li> </ul>"},{"location":"architecture/infrastructure/#cicd","title":"CI/CD","text":""},{"location":"architecture/infrastructure/#test-pipeline-on-every-push","title":"Test Pipeline (on every push)","text":"<pre><code>GitHub Actions \u2192 ruff check \u2192 mypy --strict \u2192 pytest (723 tests)\n</code></pre>"},{"location":"architecture/infrastructure/#deploy-pipeline-manual-trigger","title":"Deploy Pipeline (manual trigger)","text":"<pre><code>GitHub Actions \u2192 SSH to VPS \u2192 git pull \u2192 docker compose build\n    \u2192 up -d \u2192 alembic upgrade head \u2192 health check\n</code></pre>"},{"location":"architecture/infrastructure/#docs-pipeline-on-push-to-main-docs-changes","title":"Docs Pipeline (on push to main, docs/ changes)","text":"<pre><code>GitHub Actions \u2192 uv sync --only-group docs \u2192 mkdocs gh-deploy\n</code></pre>"},{"location":"architecture/infrastructure/#key-configuration","title":"Key Configuration","text":"<p>All production config via <code>.env.prod</code> (not committed). See <code>.env.example</code> for template.</p>"},{"location":"architecture/infrastructure/#worker-specific-variables","title":"Worker-specific variables","text":"Variable Default Description <code>REDIS_URL</code> <code>redis://localhost:6379/0</code> Redis connection string <code>WORKER_MAX_JOBS</code> <code>1</code> Concurrent jobs per worker (1 to avoid OOM with Whisper ~5 GB RAM) <code>WORKER_JOB_TIMEOUT</code> <code>21600</code> Max seconds per job (6h \u2014 enough for 4h video transcription on CPU) <code>WORKER_MAX_TRIES</code> <code>3</code> Retry attempts per job <code>WORKER_HEAVY_WINDOW_ENABLED</code> <code>false</code> Restrict heavy jobs to time window <code>WORKER_HEAVY_WINDOW_START</code> <code>02:00</code> Window start (24h format) <code>WORKER_HEAVY_WINDOW_END</code> <code>06:30</code> Window end (24h format) <code>WORKER_HEAVY_WINDOW_TZ</code> <code>UTC</code> Timezone for window <p>Always use <code>--env-file .env.prod</code></p> <p>All <code>docker compose</code> commands on the VPS must include <code>--env-file .env.prod</code> for variable interpolation in the compose file itself.</p> <p>For full deployment instructions, see Deployment Guide.</p>"},{"location":"development/deployment/","title":"Deployment Guide","text":"<p>Production deployment for Course Supporter API on a shared VPS with Docker Compose.</p> <p>Domain: <code>api.pythoncourse.me</code> Architecture: FastAPI app + PostgreSQL (pgvector) + Backblaze B2 (S3) + Netdata monitoring, behind shared nginx reverse proxy.</p>"},{"location":"development/deployment/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>VPS: Ubuntu 22.04/24.04 with Docker and Docker Compose v2 installed</li> <li>Domain: DNS A-record pointing to VPS IP (e.g. <code>api.pythoncourse.me</code>)</li> <li>Backblaze B2: account with bucket and application key (S3-compatible API)</li> <li>LLM API keys: at least one of Gemini, Anthropic, OpenAI, DeepSeek</li> <li>GitHub access: deploy SSH key (read-only) added to the repository</li> <li>Nginx: running on the VPS (shared with other services via <code>shared-net</code> Docker network)</li> <li>Optional: <code>jq</code> for JSON filtering in shell commands (<code>sudo apt install jq</code>)</li> </ul>"},{"location":"development/deployment/#2-first-deploy-from-scratch","title":"2. First Deploy (from scratch)","text":""},{"location":"development/deployment/#21-create-deploy-user","title":"2.1. Create deploy user","text":"<pre><code>sudo useradd -m -s /bin/bash deploy-course-supporter\nsudo usermod -aG docker deploy-course-supporter\n</code></pre>"},{"location":"development/deployment/#22-generate-deploy-ssh-key","title":"2.2. Generate deploy SSH key","text":"<pre><code>sudo -u deploy-course-supporter ssh-keygen -t ed25519 -f /home/deploy-course-supporter/.ssh/deploy_key -N \"\"\n</code></pre> <p>Add the public key to GitHub repository as a read-only deploy key.</p>"},{"location":"development/deployment/#23-clone-repository","title":"2.3. Clone repository","text":"<pre><code>sudo mkdir -p /opt/course-supporter\nsudo chown deploy-course-supporter:deploy-course-supporter /opt/course-supporter\n\nsudo -u deploy-course-supporter bash -c '\n  GIT_SSH_COMMAND=\"ssh -i /home/deploy-course-supporter/.ssh/deploy_key\" \\\n  git clone git@github.com:KonstZiv/course-supporter.git /opt/course-supporter\n'\n</code></pre> <p>All subsequent commands should be run as the <code>deploy-course-supporter</code> user: <code>sudo -i -u deploy-course-supporter</code></p>"},{"location":"development/deployment/#24-create-docker-network","title":"2.4. Create Docker network","text":"<pre><code>docker network inspect shared-net &gt;/dev/null 2&gt;&amp;1 || docker network create shared-net\n</code></pre> <p>This network is shared with the main nginx proxy. The command is a no-op if it already exists.</p>"},{"location":"development/deployment/#25-configure-environment","title":"2.5. Configure environment","text":"<pre><code>cd /opt/course-supporter\ncp .env.prod.example .env.prod\n# Edit .env.prod with actual credentials:\nnano .env.prod\n</code></pre> <p>See Environment Variables Reference for all variables.</p>"},{"location":"development/deployment/#26-build-and-start-services","title":"2.6. Build and start services","text":"<pre><code>cd /opt/course-supporter\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml build\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml up -d\n</code></pre>"},{"location":"development/deployment/#27-run-database-migrations","title":"2.7. Run database migrations","text":"<pre><code>docker compose --env-file .env.prod -f docker-compose.prod.yaml exec -T app \\\n  python -m alembic upgrade head\n</code></pre> <p>Note: Use <code>python -m alembic</code> instead of bare <code>alembic</code> \u2014 uv generates entry-point scripts with shebangs pointing to the builder stage path, which doesn't exist in the runtime image.</p>"},{"location":"development/deployment/#28-configure-nginx","title":"2.8. Configure nginx","text":"<p>Add a server block for <code>api.pythoncourse.me</code>. Full reference: <code>deploy/nginx/course-supporter.conf</code>.</p> <p>Key parts:</p> <pre><code># HTTP \u2192 HTTPS redirect\nserver {\n    listen 80;\n    server_name api.pythoncourse.me;\n\n    location /.well-known/acme-challenge/ {\n        root /var/www/html;\n    }\n\n    location / {\n        return 301 https://$host$request_uri;\n    }\n}\n\n# HTTPS\nserver {\n    listen 443 ssl;\n    server_name api.pythoncourse.me;\n\n    ssl_certificate /etc/letsencrypt/live/api.pythoncourse.me/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/api.pythoncourse.me/privkey.pem;\n    ssl_protocols TLSv1.2 TLSv1.3;\n\n    client_max_body_size 1G;\n    # Stream uploads directly to upstream without disk buffering (video files up to 1GB)\n    proxy_request_buffering off;\n\n    # Docker DNS \u2014 resolve at request time, not at startup\n    resolver 127.0.0.11 valid=30s;\n    set $course_supporter http://course-supporter-app:8000;\n    set $netdata_backend http://netdata:19999;\n\n    location / {\n        proxy_pass $course_supporter;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n    location /netdata/ {\n        auth_basic \"Monitoring\";\n        auth_basic_user_file /etc/nginx/.htpasswd_netdata;\n        proxy_pass $netdata_backend/;\n        proxy_set_header Host $host;\n    }\n}\n</code></pre> <p>See <code>deploy/nginx/course-supporter.conf</code> for TLS hardening, security headers, and upload timeouts.</p> <p>Reload nginx:</p> <pre><code># Nginx runs as a separate container (not part of docker-compose.prod.yaml)\ndocker exec nginx nginx -s reload\n</code></pre>"},{"location":"development/deployment/#29-ssl-certificate","title":"2.9. SSL certificate","text":"<pre><code>certbot certonly --webroot -w /var/www/html -d api.pythoncourse.me\n</code></pre> <p>Update nginx config with certificate paths, then reload:</p> <pre><code># Nginx runs as a separate container (not part of docker-compose.prod.yaml)\ndocker exec nginx nginx -s reload\n</code></pre>"},{"location":"development/deployment/#210-create-first-tenant","title":"2.10. Create first tenant","text":"<pre><code>docker compose --env-file .env.prod -f docker-compose.prod.yaml exec app \\\n  python -m scripts.manage_tenant create-tenant --name \"MyCompany\"\n\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml exec app \\\n  python -m scripts.manage_tenant create-key \\\n    --tenant \"MyCompany\" --scopes prep,check --label production\n</code></pre> <p>The API key is displayed once \u2014 save it immediately.</p>"},{"location":"development/deployment/#211-verify-deployment","title":"2.11. Verify deployment","text":"<pre><code># Health check:\ncurl -sf https://api.pythoncourse.me/health | jq .\n\n# Auth check:\ncurl -H \"X-API-Key: cs_live_...\" https://api.pythoncourse.me/api/v1/courses\n</code></pre>"},{"location":"development/deployment/#3-subsequent-deploys","title":"3. Subsequent Deploys","text":""},{"location":"development/deployment/#automated-github-actions","title":"Automated (GitHub Actions)","text":"<p>Trigger manually via Actions &gt; Deploy &gt; Run workflow in GitHub.</p> <p>The workflow: 1. SSH into VPS 2. <code>git pull origin main</code> 3. Build app image (<code>docker compose --env-file .env.prod</code>) 4. Start all containers (<code>up -d</code>) 5. Run migrations (<code>python -m alembic upgrade head</code>) 6. Health check via <code>docker exec</code> (10 attempts, 5s interval)</p> <p>Note: Health check runs inside the app container via <code>docker exec</code> because port 8000 is not exposed to the host \u2014 Nginx proxies via <code>shared-net</code> Docker network.</p>"},{"location":"development/deployment/#manual","title":"Manual","text":"<pre><code>cd /opt/course-supporter\ngit pull origin main\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml build app worker\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml up -d\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml exec -T app \\\n  python -m alembic upgrade head\ncurl -sf https://api.pythoncourse.me/health | jq .\n</code></pre>"},{"location":"development/deployment/#4-environment-variables-reference","title":"4. Environment Variables Reference","text":"Variable Required Default Description <code>ENVIRONMENT</code> Yes (Must be set) <code>production</code> for JSON logging and debug off <code>LOG_LEVEL</code> No <code>INFO</code> Python log level: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code> <code>CORS_ALLOWED_ORIGINS</code> No <code>[]</code> JSON list of allowed origins, e.g. <code>[\"https://pythoncourse.me\"]</code> <code>CORS_ALLOW_CREDENTIALS</code> No <code>false</code> Allow credentials in CORS <code>CORS_ALLOWED_METHODS</code> No <code>[\"GET\",\"POST\"]</code> Allowed HTTP methods <code>CORS_ALLOWED_HEADERS</code> No <code>[\"Content-Type\",\"X-API-Key\"]</code> Allowed request headers <code>POSTGRES_USER</code> No <code>course_supporter</code> PostgreSQL user <code>POSTGRES_PASSWORD</code> Yes \u2014 PostgreSQL password <code>POSTGRES_DB</code> No <code>course_supporter</code> PostgreSQL database name <code>POSTGRES_HOST</code> Yes \u2014 PostgreSQL host (<code>postgres-cs</code> in Docker) <code>POSTGRES_PORT</code> No <code>5432</code> PostgreSQL port <code>S3_ENDPOINT</code> Yes \u2014 S3-compatible endpoint URL <code>S3_ACCESS_KEY</code> Yes \u2014 S3 access key ID <code>S3_SECRET_KEY</code> Yes \u2014 S3 secret key <code>S3_BUCKET</code> Yes <code>course-materials</code> S3 bucket name <code>GEMINI_API_KEY</code> No \u2014 Google Gemini API key <code>ANTHROPIC_API_KEY</code> No \u2014 Anthropic API key <code>OPENAI_API_KEY</code> No \u2014 OpenAI API key <code>DEEPSEEK_API_KEY</code> No \u2014 DeepSeek API key <code>REDIS_URL</code> No <code>redis://localhost:6379/0</code> Redis connection string (<code>redis://redis:6379/0</code> in Docker) <code>WORKER_MAX_JOBS</code> No <code>1</code> Concurrent jobs per worker. Default 1 to avoid OOM (Whisper uses ~5 GB RAM) <code>WORKER_JOB_TIMEOUT</code> No <code>21600</code> Max seconds per job (6h \u2014 enough for 4h video transcription on CPU) <code>WORKER_MAX_TRIES</code> No <code>3</code> Retry attempts per failed job <code>WORKER_HEAVY_WINDOW_ENABLED</code> No <code>false</code> Restrict heavy jobs to a time window <code>WORKER_HEAVY_WINDOW_START</code> No <code>02:00</code> Window start (24h format) <code>WORKER_HEAVY_WINDOW_END</code> No <code>06:30</code> Window end (24h format) <code>WORKER_HEAVY_WINDOW_TZ</code> No <code>UTC</code> Timezone for the work window <p>At least one LLM API key is required for the ArchitectAgent to function.</p>"},{"location":"development/deployment/#5-tenant-management","title":"5. Tenant Management","text":"<p>All commands run inside the app container:</p> <pre><code># Prefix for all commands:\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml exec app python -m scripts.manage_tenant\n</code></pre> Command Options Description <code>create-tenant</code> <code>--name \"Company\"</code> Create a new tenant <code>create-key</code> <code>--tenant \"Company\" --scopes prep,check --label prod</code> Generate API key (shown once) <code>list-tenants</code> \u2014 List all tenants with status and key counts <code>list-keys</code> <code>--tenant \"Company\"</code> List keys for a tenant <code>revoke-key</code> <code>--prefix cs_live_abc1</code> Revoke a specific API key <code>deactivate-tenant</code> <code>--name \"Company\"</code> Deactivate tenant (all keys become invalid) <p>Optional rate limit flags for <code>create-key</code>: - <code>--rate-prep 60</code> \u2014 requests per minute for prep scope - <code>--rate-check 300</code> \u2014 requests per minute for check scope</p>"},{"location":"development/deployment/#6-monitoring","title":"6. Monitoring","text":""},{"location":"development/deployment/#health-check","title":"Health check","text":"<pre><code>curl -sf https://api.pythoncourse.me/health | jq .\n# Returns: {\"status\": \"ok\", \"checks\": {\"db\": \"ok\", \"s3\": \"ok\", \"redis\": \"ok\"}, \"timestamp\": \"...\"}\n# HTTP 200 = healthy, HTTP 503 = degraded\n</code></pre>"},{"location":"development/deployment/#netdata-dashboard","title":"Netdata dashboard","text":"<ul> <li>URL: <code>https://api.pythoncourse.me/netdata/</code></li> <li>Protected by HTTP basic auth (<code>.htpasswd_netdata</code>)</li> <li>Monitors: CPU, RAM, disk, Docker containers, network I/O</li> </ul> <p>Setup Telegram alerts \u2014 see <code>current-doc/S2-prod-deploy/infrastructure/netdata-setup.md</code>.</p>"},{"location":"development/deployment/#application-logs","title":"Application logs","text":"<pre><code># Follow logs:\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml logs -f app\n\n# Filter errors (JSON format in production):\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml logs app --no-log-prefix | jq 'select(.level == \"error\")'\n\n# Last 50 lines:\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml logs app --tail=50\n\n# Worker logs (job processing):\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml logs -f worker\n</code></pre>"},{"location":"development/deployment/#external-monitoring","title":"External monitoring","text":"<p>Configure UptimeRobot (or similar) to ping <code>https://api.pythoncourse.me/health</code> every 5 minutes.</p>"},{"location":"development/deployment/#7-backup-restore","title":"7. Backup &amp; Restore","text":""},{"location":"development/deployment/#database-backup","title":"Database backup","text":"<pre><code>docker compose --env-file .env.prod -f docker-compose.prod.yaml exec postgres-cs \\\n  pg_dump -U course_supporter course_supporter &gt; backup_$(date +%Y%m%d).sql\n</code></pre>"},{"location":"development/deployment/#database-restore","title":"Database restore","text":"<pre><code>cat backup.sql | docker compose --env-file .env.prod -f docker-compose.prod.yaml exec -T postgres-cs \\\n  psql -U course_supporter course_supporter\n</code></pre> <p>S3 data (Backblaze B2) is managed externally and has its own versioning/lifecycle policies.</p>"},{"location":"development/deployment/#8-rollback","title":"8. Rollback","text":""},{"location":"development/deployment/#application-rollback","title":"Application rollback","text":"<pre><code>cd /opt/course-supporter\ngit log --oneline -5                    # find target commit\ngit checkout &lt;commit-hash&gt;\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml build app\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml up -d app\ncurl -sf https://api.pythoncourse.me/health | jq .\n</code></pre>"},{"location":"development/deployment/#migration-rollback","title":"Migration rollback","text":"<pre><code>docker compose --env-file .env.prod -f docker-compose.prod.yaml exec -T app \\\n  python -m alembic downgrade -1\n</code></pre> <p>Always verify the target migration before running downgrade in production.</p>"},{"location":"development/deployment/#9-troubleshooting","title":"9. Troubleshooting","text":"Problem Diagnosis Solution 502 Bad Gateway <code>docker compose --env-file .env.prod -f docker-compose.prod.yaml ps</code> \u2014 is app running? <code>docker compose --env-file .env.prod -f docker-compose.prod.yaml up -d app</code> DB connection refused <code>docker compose --env-file .env.prod -f docker-compose.prod.yaml logs postgres-cs</code> Check <code>POSTGRES_HOST=postgres-cs</code> in <code>.env.prod</code> S3 upload failure Health check shows <code>\"s3\": \"error: ...\"</code> Verify B2 credentials and endpoint in <code>.env.prod</code> SSL certificate expired <code>certbot certificates</code> <code>certbot renew &amp;&amp; docker exec nginx nginx -s reload</code> OOM kill <code>docker compose --env-file .env.prod -f docker-compose.prod.yaml ps -q app \\| xargs docker inspect \\| jq '.[0].State.OOMKilled'</code> Reduce workers, check memory-heavy operations Slow responses Check app logs for <code>latency_ms</code> values Review LLM provider latency, check DB query times Rate limit hit (429) Response includes <code>Retry-After</code> header Wait or adjust rate limits via <code>create-key</code> Health check fails after DB recreate <code>health_check_db_error: OperationalError</code> in logs App holds stale DB connection \u2014 restart app: <code>docker compose --env-file .env.prod -f docker-compose.prod.yaml restart app</code> Worker not processing jobs <code>docker compose ... logs worker --tail=20</code> Check <code>REDIS_URL</code> in <code>.env.prod</code>, verify Redis is healthy Job timeout on long video <code>worker_job_timeout</code> too low for Whisper on CPU Increase <code>WORKER_JOB_TIMEOUT</code> (default 21600 = 6h, enough for ~4h video) Worker OOM killed <code>docker inspect ... \\| jq '.[0].State.OOMKilled'</code> Reduce <code>WORKER_MAX_JOBS</code> to 1 (default). Whisper <code>medium</code> uses ~5 GB RAM <code>alembic: no such file or directory</code> uv shebang points to builder stage path Use <code>python -m alembic</code> instead of bare <code>alembic</code>"},{"location":"development/deployment/#useful-debug-commands","title":"Useful debug commands","text":"<pre><code># Container status:\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml ps\n\n# Database shell:\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml exec postgres-cs \\\n  psql -U course_supporter course_supporter\n\n# App shell:\ndocker compose --env-file .env.prod -f docker-compose.prod.yaml exec app /bin/bash\n\n# Check disk space:\ndf -h /var/lib/docker\n</code></pre>"},{"location":"development/setup/","title":"Development Setup","text":"<p>Coming in Sprint 2, Epic 7</p> <p>Full setup guide will be published here.</p> <p>Quick start:</p> <pre><code># Clone and install\ngit clone &lt;repo-url&gt;\ncd course-supporter\nuv sync                       # dev deps included by default (PEP 735)\nuv run pre-commit install\n\n# Start infrastructure\ndocker compose up -d           # PostgreSQL + MinIO\n\n# Copy env and fill in API keys\ncp .env.example .env\n\n# Run migrations\nmake db-upgrade\n\n# Run checks\nmake check                     # ruff + mypy + pytest\n\n# Start API server\nuv run uvicorn course_supporter.api:app --reload\n</code></pre> <p>See also: Infrastructure | Deployment</p>"},{"location":"development/testing/","title":"Testing","text":"<p>Coming in Sprint 2, Epic 7</p> <p>Detailed testing strategy and patterns will be published here.</p> <p>Current setup:</p> <ul> <li>Framework: pytest with <code>pytest-asyncio</code> (<code>asyncio_mode = \"auto\"</code>)</li> <li>Test count: 407 (Sprint 0: 326, Sprint 1: +81)</li> <li>Coverage: <code>pytest --cov --cov-report=term-missing</code></li> </ul> <p>Key patterns:</p> <ul> <li><code>httpx.AsyncClient</code> + <code>ASGITransport(app=app)</code> for API tests</li> <li>Patch <code>PROCESSOR_MAP</code> dict directly for ingestion tests</li> <li><code>MagicMock(return_value=ctx)</code> for <code>async_sessionmaker</code> mock</li> <li>Fixtures over classes (<code>pytest</code> style)</li> </ul> <p>Commands:</p> <pre><code>uv run pytest                           # all tests\nuv run pytest tests/unit/test_config.py # single file\nuv run pytest -k \"test_name\"            # by name\nmake check                              # full: ruff + mypy + pytest\n</code></pre>"},{"location":"sprints/","title":"Sprint Roadmap","text":"Sprint Name Status Tests 0 Materials-to-Structure MVP Complete 326 1 Production Deploy Complete 407 2 Material Tree, Task Queue, Structure Generation In Progress \u2014"},{"location":"sprints/sprint-0/","title":"Sprint 0 \u2014 Materials-to-Structure MVP","text":"<p>Status: Complete Duration: ~3 weeks Tests: 326</p>"},{"location":"sprints/sprint-0/#goal","title":"Goal","text":"<p>Build a complete pipeline from uploading course materials (video, presentations, text, web links) to generating a structured course plan with timecodes, slides, concepts, and exercises.</p> <p>Demo: <code>POST /courses</code> on a real Python tutorial produces a full course structure.</p>"},{"location":"sprints/sprint-0/#epics","title":"Epics","text":""},{"location":"sprints/sprint-0/#epic-1-project-bootstrap-zero-to-make-check","title":"Epic 1: Project Bootstrap (\"Zero to <code>make check</code>\")","text":"<p>Goal: After <code>git clone</code>, a developer runs <code>make install &amp;&amp; make up &amp;&amp; make check</code> and gets a fully working environment with DB, linting, tests, and CI. No business logic \u2014 infrastructure only.</p> <p>Deliverables:</p> <ul> <li><code>uv init</code> + src layout (<code>src/course_supporter/</code>)</li> <li><code>ruff</code> + <code>mypy --strict</code> + <code>pre-commit</code> hooks</li> <li>Docker Compose: <code>pgvector:pg17</code> + MinIO</li> <li>Pydantic Settings with <code>SecretStr</code> for API keys</li> <li>Alembic (sync template, psycopg v3) with 8 tables, UUIDv7 PKs</li> <li>GitHub Actions CI: lint \u2192 typecheck \u2192 test</li> </ul> <p>Results: 6 tasks (S1-001 \u2013 S1-006), 17 tests.</p>"},{"location":"sprints/sprint-0/#epic-2-model-registry-llm-infrastructure","title":"Epic 2: Model Registry &amp; LLM Infrastructure","text":"<p>Goal: Unified interface to 4 LLM providers (Gemini, Anthropic, OpenAI, DeepSeek) with strategy-based routing. Any component calls LLM through <code>ModelRouter</code> without knowing provider details.</p> <p>Deliverables:</p> <ul> <li>ABC <code>LLMProvider</code> with 3 implementations (Gemini, Anthropic, OpenAI \u2014 DeepSeek uses OpenAI SDK with custom <code>base_url</code>)</li> <li><code>config/models.yaml</code> \u2014 5 models, 4 actions, 3 strategies</li> <li><code>ModelRouter</code> with two-level fallback (within chain + cross-strategy) and error classification (permanent vs transient)</li> <li><code>LogCallback</code> for automatic DB logging of every call with cost/tokens</li> <li>One-stop factory <code>create_model_router()</code></li> </ul> <p>Results: 4 tasks (S1-007 \u2013 S1-010), 67 tests.</p>"},{"location":"sprints/sprint-0/#epic-3-ingestion-engine","title":"Epic 3: Ingestion Engine","text":"<p>Goal: Accept video, PDF/PPTX, text (MD/DOCX/HTML), URLs and transform each source into a unified <code>SourceDocument</code>. Multiple sources merge into <code>CourseContext</code>.</p> <p>Deliverables:</p> <ul> <li>ABC <code>SourceProcessor</code> + Pydantic schemas (7 <code>ChunkType</code> variants)</li> <li><code>VideoProcessor</code> \u2014 Gemini primary + Whisper fallback via composition pattern</li> <li><code>PresentationProcessor</code> \u2014 PDF + PPTX + optional Vision LLM for slide descriptions</li> <li><code>TextProcessor</code> \u2014 markdown/docx/html, no LLM required</li> <li><code>WebProcessor</code> \u2014 trafilatura extraction, no LLM required</li> <li><code>MergeStep</code> \u2014 sync cross-references (slides \u2194 timecodes)</li> <li><code>SourceMaterialRepository</code> with status machine (pending \u2192 processing \u2192 done/error)</li> </ul> <p>Results: 8 tasks (S1-011 \u2013 S1-018), 101 tests.</p>"},{"location":"sprints/sprint-0/#epic-4-architect-agent","title":"Epic 4: Architect Agent","text":"<p>Goal: AI agent that analyzes <code>CourseContext</code> and generates a structured curriculum: modules \u2192 lessons \u2192 concepts with cross-references + exercises.</p> <p>Deliverables:</p> <ul> <li>7 Pydantic output models (<code>CourseStructure</code>, <code>ModuleOutput</code>, <code>LessonOutput</code>, etc.) with learning-oriented fields (goal, knowledge, skills, difficulty)</li> <li>Pedagogical system prompt v1 in YAML with <code>PromptData</code> Pydantic model</li> <li>Step-based <code>ArchitectAgent</code> (<code>_prepare_prompts</code> \u2192 <code>_generate</code>)</li> <li><code>CourseStructureRepository</code> with replace strategy (clear + cascade delete)</li> </ul> <p>Results: 4 tasks (S1-019 \u2013 S1-022), 55 tests.</p>"},{"location":"sprints/sprint-0/#epic-5-api-layer","title":"Epic 5: API Layer","text":"<p>Goal: REST API \u2014 the system's face. Via HTTP: create course, upload materials, run Ingestion + ArchitectAgent, retrieve structure.</p> <p>Deliverables:</p> <ul> <li>FastAPI app with lifespan (DB pool + ModelRouter + S3Client)</li> <li>5 endpoints: <code>POST /courses</code>, <code>POST /materials</code>, <code>POST /slide-mapping</code>, <code>GET /courses/{id}</code>, <code>GET /lessons/{id}</code></li> <li>Async <code>S3Client</code> (aiobotocore) for object storage</li> <li>Background <code>ingest_material</code> task with <code>PROCESSOR_MAP</code></li> <li><code>CourseRepository</code>, <code>SlideVideoMappingRepository</code>, <code>LessonRepository</code></li> </ul> <p>Results: 6 tasks (S1-023 \u2013 S1-028), 54 tests.</p>"},{"location":"sprints/sprint-0/#epic-6-evals-observability","title":"Epic 6: Evals &amp; Observability","text":"<p>Goal: Tools for evaluating generation quality and monitoring: structured logging, eval pipeline, cost reporting.</p> <p>Deliverables:</p> <ul> <li><code>configure_logging()</code> \u2014 JSON for prod, console for dev + <code>RequestLoggingMiddleware</code></li> <li>Test dataset (Python basics: transcript, slides, tutorial)</li> <li>Reference gold standard <code>CourseStructure</code> (3 modules, 6 lessons, 13 concepts)</li> <li><code>StructureComparator</code> (5 weighted metrics, fuzzy matching via <code>SequenceMatcher</code>)</li> <li>Dual-mode eval CLI (<code>--mock</code> for CI, real LLM by default)</li> <li><code>LLMCallRepository</code> with SQL aggregation + cost report API endpoint</li> </ul> <p>Results: 5 tasks (S1-029 \u2013 S1-033), 32 tests.</p>"},{"location":"sprints/sprint-0/#key-architecture-decisions","title":"Key Architecture Decisions","text":"Decision Rationale psycopg v3 only (<code>postgresql+psycopg://</code>) Modern async driver, Alembic sync template works with both sync/async UUIDv7 via <code>uuid-utils</code> Time-ordered, sortable, no sequence bottleneck PEP 735 dependency groups <code>[dependency-groups]</code> for dev tools, <code>[project.optional-dependencies]</code> for media (Whisper + PyTorch ~2GB) Strategy-based <code>ModelRouter</code> Two-level fallback, permanent vs transient error classification Composition pattern for <code>VideoProcessor</code> Gemini + Whisper as separate classes, shell orchestrator Repository <code>flush()</code> not <code>commit()</code> Caller controls transaction boundary <code>selectinload</code> chains Avoids cartesian product (vs <code>joinedload</code>) Step-based <code>ArchitectAgent</code> Ready for LangGraph/DAG migration"},{"location":"sprints/sprint-0/#results","title":"Results","text":"<ul> <li>6 epics, 33 tasks \u2014 all complete</li> <li>326 tests, <code>make check</code> green</li> <li>3 Alembic migrations (initial schema + action/strategy refactor + learning fields)</li> <li>~4890 LOC source code</li> </ul>"},{"location":"sprints/sprint-0/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Router type safety \u2014 needs <code>@overload</code> decorators for <code>_execute_with_fallback</code> instead of <code>type: ignore[return-value]</code></li> <li><code>PROVIDER_CONFIG</code> string-based \u2014 fragile <code>getattr(settings, config[\"key_attr\"])</code>; refactor to <code>@dataclass ProviderFactoryConfig</code> with <code>Callable</code></li> <li><code>error</code> as terminal state \u2014 need <code>error \u2192 pending</code> transition for retry workflow</li> <li>Empty <code>models/llm.py</code> \u2014 leftover from early stage (removed in Sprint 1)</li> <li>CORS <code>[\"*\"]</code> \u2014 addressed in Sprint 1 (PD-017 security hardening)</li> </ol>"},{"location":"sprints/sprint-0/#what-stayed-out-of-scope","title":"What Stayed Out of Scope","text":"<ul> <li>Background task queue (deferred to Sprint 2 \u2014 ARQ + Redis)</li> <li>Integration tests with real DB</li> <li>RAG / embeddings search</li> <li>Student model, submissions</li> <li>Automatic slide-video mapping (vision-based)</li> <li>Frontend / UI</li> <li>Authentication / authorization (done in Sprint 1)</li> </ul>"},{"location":"sprints/sprint-1/","title":"Sprint 1 \u2014 Production Deploy","text":"<p>Status: Complete Duration: ~2 weeks Tests: 407 (56 new in Epic 1) Deployed: 2026-02-18</p>"},{"location":"sprints/sprint-1/#goal","title":"Goal","text":"<p>Deploy Course Supporter API to a production VPS with multi-tenant auth, rate limiting, S3 storage, and automated deploy. Result: a live <code>api.pythoncourse.me</code> with Swagger UI, ready to accept requests from tenant clients.</p>"},{"location":"sprints/sprint-1/#epics","title":"Epics","text":""},{"location":"sprints/sprint-1/#epic-1-multi-tenant-auth-56-tests","title":"Epic 1: Multi-tenant &amp; Auth (56 tests)","text":"<p>Goal: Build B2B API foundation \u2014 data isolation per tenant, API key authentication, rate limiting per service scope (prep/check).</p> Task Description PD-001 Tenant &amp; API Key ORM models (<code>tenants</code> + <code>api_keys</code> tables, Alembic migration) PD-002 <code>tenant_id</code> on existing tables (<code>courses.tenant_id</code>, <code>llm_calls.tenant_id</code>) PD-003 API Key auth middleware (header \u2192 SHA-256 hash \u2192 DB lookup \u2192 tenant context) PD-004 Service scope enforcement (<code>prep</code> / <code>check</code> scopes at endpoint level) PD-005 Rate limiting middleware (in-memory sliding window per tenant + scope) PD-006 Tenant-scoped repositories (<code>CourseRepository</code> filters by <code>tenant_id</code>) PD-007 Admin CLI (<code>manage_tenant.py</code>: create tenant, issue/revoke keys) <p>Key decisions:</p> <ul> <li>API key format: <code>cs_live_&lt;32 hex chars&gt;</code>, only SHA-256 hash stored in DB</li> <li><code>TenantContext</code> frozen dataclass \u2014 <code>tenant_id</code>, <code>tenant_name</code>, <code>scopes</code>, <code>rate_limits</code></li> <li><code>Annotated</code> deps (<code>PrepDep</code>, <code>SharedDep</code>) for typed scope dependencies</li> <li>In-memory rate limiter (single-process); Redis planned for horizontal scaling</li> <li><code>LLMCall.tenant_id</code> nullable \u2014 background tasks and evals have no tenant context</li> </ul>"},{"location":"sprints/sprint-1/#epic-2-production-docker-infrastructure","title":"Epic 2: Production Docker &amp; Infrastructure","text":"<p>Goal: Deploy to VPS: Dockerfile, production compose, nginx routing, Backblaze B2, streaming upload up to 1GB, health checks, Netdata monitoring.</p> Task Description PD-008 Dockerfile (multi-stage: builder + runtime, python:3.13-slim, non-root user) PD-009 docker-compose.prod.yaml (app + postgres-cs + netdata, shared-net) PD-010 Nginx config for <code>api.pythoncourse.me</code> (proxy via shared-net, security headers) PD-011 SSL certificate (certbot, Let's Encrypt, auto-renewal) PD-012 Backblaze B2 integration (S3-compatible, credentials in env) PD-013 Streaming upload 1GB (S3 multipart, 10MB parts, ~10-20 MB RAM) PD-014 Deep health check (<code>/health</code> checks DB + S3, returns 200/503) PD-015 Monitoring \u2014 Netdata (dashboard with basic auth, alerts \u2192 Telegram) <p>Key decisions:</p> <ul> <li>App/netdata ports NOT exposed to host \u2014 all traffic through nginx in shared Docker network</li> <li><code>proxy_request_buffering off</code> \u2014 nginx streams directly to upstream</li> <li>Three monitoring layers: Netdata (system), <code>/health</code> (app), UptimeRobot (external)</li> <li>2 uvicorn workers (not 4) to conserve RAM on shared VPS</li> </ul>"},{"location":"sprints/sprint-1/#epic-3-cicd-hardening","title":"Epic 3: CI/CD &amp; Hardening","text":"<p>Goal: Automated deploy via GitHub Actions, security hardening, production logging, deploy documentation, smoke test.</p> Task Description PD-016 GitHub Actions deploy workflow (SSH deploy via <code>appleboy/ssh-action</code>) PD-017 Security hardening (CORS restricted, security headers, no debug in prod) PD-018 Production logging config (JSON structured logs, structlog) PD-019 Deploy documentation (full deployment guide with troubleshooting) PD-020 Smoke test script (post-deploy: health + auth + basic CRUD) <p>Key decisions:</p> <ul> <li>Deploy pipeline: <code>git pull</code> \u2192 <code>docker compose build</code> \u2192 <code>up -d</code> \u2192 <code>alembic upgrade head</code> (build on VPS, no Docker registry)</li> <li>Security headers in nginx: HSTS, X-Content-Type-Options, X-Frame-Options, X-XSS-Protection, Referrer-Policy</li> <li>CORS \u2014 principle of least privilege; prod overrides via <code>.env.prod</code></li> </ul>"},{"location":"sprints/sprint-1/#production-infrastructure","title":"Production Infrastructure","text":"Component Details VPS 8 GB RAM, 2 vCPU (Xeon Gold), 32 GB disk Domain <code>api.pythoncourse.me</code> TLS Let's Encrypt (TLSv1.2/1.3, HSTS, OCSP) Database PostgreSQL 17 (<code>pgvector/pgvector:pg17</code>), named volume <code>pgdata-cs</code> Object Storage Backblaze B2 (S3-compatible), bucket <code>course-supporter</code> Monitoring Netdata (dashboard + Telegram alerts), deep <code>/health</code> endpoint, UptimeRobot Deploy path <code>/opt/course-supporter</code> on VPS Containers <code>course-supporter-app</code>, <code>course-supporter-db</code>, <code>netdata</code>"},{"location":"sprints/sprint-1/#key-architecture-decisions","title":"Key Architecture Decisions","text":"Decision Rationale API key auth (not OAuth/JWT) Simple B2B model, stateless, easy to rotate per tenant SHA-256 hash storage Raw key never stored; only hash + prefix for identification In-memory rate limiter Sufficient for single-process; Redis upgrade path clear <code>TenantContext</code> via <code>Depends()</code> Frozen dataclass injected into every endpoint, zero boilerplate Shared Docker network for nginx No port exposure, nginx resolves containers by name S3 multipart upload (10MB parts) Constant ~10-20 MB RAM for files up to 1GB Build on VPS (no registry) Simplest pipeline for single-server deployment"},{"location":"sprints/sprint-1/#results","title":"Results","text":"<ul> <li>3 epics, 20 tasks (PD-001 \u2013 PD-020) \u2014 all complete</li> <li>407 tests, <code>make check</code> green</li> <li>API live: <code>https://api.pythoncourse.me/health</code> \u2192 status ok (DB ok, S3 ok)</li> <li>Tenant: <code>pythoncourse</code> created via Admin CLI</li> <li>CI/CD: push to main \u2192 GitHub Actions \u2192 auto-deploy \u2192 live</li> </ul>"},{"location":"sprints/sprint-1/#lessons-learned","title":"Lessons Learned","text":"<ol> <li><code>--env-file .env.prod</code> is a Docker Compose CLI flag for variable interpolation in the compose file itself; the <code>env_file:</code> directive only sets variables inside the container</li> <li><code>UV_LINK_MODE=copy</code> needed in Dockerfile to avoid broken symlinks when copying venv between stages</li> <li>Shebang issue \u2014 uv generates scripts with shebang pointing to builder stage path; use <code>python -m &lt;tool&gt;</code> instead of direct binary (<code>uvicorn</code>, <code>alembic</code>, <code>yt-dlp</code>)</li> <li><code>PYTHONPATH=/app/src</code> needed for src layout when using <code>--no-install-project</code></li> <li><code>script_stop</code> is NOT a valid param for <code>appleboy/ssh-action@v1</code>; use <code>set -e</code> in script</li> <li>Health check via <code>docker exec</code> \u2014 port 8000 not exposed to host; nginx proxies via shared-net</li> <li>DB container recreate \u2014 if DB container is recreated but app stays running, stale connections cause <code>OperationalError</code>; must restart app</li> <li>Nginx resolver pattern \u2014 <code>resolver 127.0.0.11 valid=30s; set $var http://container:port;</code> for dynamic DNS resolution in Docker</li> </ol>"},{"location":"sprints/sprint-2/","title":"Sprint 2 \u2014 Material Tree, Task Queue, Structure Generation","text":"<p>Status: In Progress Estimate: 4-5 weeks Previous sprint: Sprint 1 \u2014 Production Deploy (407 tests, live on <code>api.pythoncourse.me</code>) Current test count: 783 passed, 27 skipped</p>"},{"location":"sprints/sprint-2/#goal","title":"Goal","text":"<p>Intuitive course material flow + production-ready processing with a task queue + per-node structure generation.</p>"},{"location":"sprints/sprint-2/#motivation","title":"Motivation","text":"<p>Sprint 1 delivered a working MVP, but:</p> <ol> <li>No intuitive flow \u2014 flat list of materials, no hierarchy, no explicit generation trigger</li> <li>Fire-and-forget processing \u2014 <code>BackgroundTasks</code> without queue, no concurrency control, tasks lost on restart</li> <li>Heavy ops not isolated \u2014 whisper/vision baked into processors, impossible to extract to serverless</li> <li>No version control \u2014 unclear if course structure matches the current set of materials</li> <li>External team waiting \u2014 needs documentation of flow + endpoints to start integration</li> </ol>"},{"location":"sprints/sprint-2/#architecture-decisions","title":"Architecture Decisions","text":""},{"location":"sprints/sprint-2/#ar-1-materialtree-recursive-adjacency-list","title":"AR-1: MaterialTree (recursive adjacency list)","text":"<p>Arbitrary hierarchy of nodes. Materials can belong to any node (not just leaves):</p> <pre><code>Course \"Python for Beginners\"\n  \u251c\u2500\u2500 syllabus.pdf                    \u2190 material attached to a root node\n  \u251c\u2500\u2500 \"Intro to Python\"\n  \u2502     \u251c\u2500\u2500 intro-video.mp4           \u2190 material at section level\n  \u2502     \u251c\u2500\u2500 \"Data Types\"\n  \u2502     \u2502     \u251c\u2500\u2500 types-slides.pdf\n  \u2502     \u2502     \u2514\u2500\u2500 types-article.html\n  \u2502     \u2514\u2500\u2500 \"Loops\"\n  \u2502           \u2514\u2500\u2500 loops-video.mp4\n  \u2514\u2500\u2500 \"Web Development\"\n        \u2514\u2500\u2500 django-overview.pdf\n</code></pre> <p>ORM: <code>MaterialNode(id, course_id, parent_id \u2192 self, title, description, order)</code>. A course has one or more root nodes (<code>parent_id = NULL</code>). Materials at \"course level\" are attached to these root nodes.</p> <p>Fixed levels (Course \u2192 Module \u2192 Topic) don't cover arbitrary learning constructs. Adjacency list is the simplest implementation for async SQLAlchemy. PostgreSQL <code>WITH RECURSIVE</code> available for complex queries, but eager loading sufficient for typical depths of 3\u20135 levels.</p>"},{"location":"sprints/sprint-2/#ar-2-materialentry-replaces-sourcematerial","title":"AR-2: MaterialEntry (replaces SourceMaterial)","text":"<p>Instead of a single <code>SourceMaterial</code> mixing raw, processed, and status \u2014 separation into clear layers with a \"receipt\" for processing submission:</p> <ul> <li>Raw layer \u2014 <code>source_url</code>, <code>filename</code>, <code>raw_hash</code> (lazy cached sha256), <code>raw_size_bytes</code></li> <li>Processed layer \u2014 <code>processed_content</code> (SourceDocument JSON), <code>processed_hash</code>, <code>processed_at</code></li> <li>Pending receipt \u2014 <code>pending_job_id</code> FK \u2192 jobs, <code>pending_since</code></li> <li>State \u2014 derived property: <code>RAW</code> \u2192 <code>PENDING</code> \u2192 <code>READY</code> / <code>INTEGRITY_BROKEN</code> / <code>ERROR</code></li> </ul> <p>The pending receipt enables diagnostics: <code>pending_since = 40 min ago</code> \u2192 suspicious \u2192 <code>GET /jobs/{pending_job_id}</code> \u2192 see the problem.</p>"},{"location":"sprints/sprint-2/#ar-3-task-queue-arq-redis","title":"AR-3: Task Queue (ARQ + Redis)","text":"<p>Replace <code>BackgroundTasks</code> with ARQ:</p> <ul> <li>Redis for persistence (jobs survive restarts)</li> <li><code>max_jobs</code> for concurrency control (whisper is CPU/RAM heavy)</li> <li>Retry with backoff for transient errors</li> <li>Job status tracking (queued \u2192 active \u2192 complete/failed)</li> <li>Job dependencies (<code>depends_on</code> \u2014 structure generation waits for ingestion)</li> <li>Work window \u2014 heavy jobs (whisper, vision) only during configured time window; light jobs (fingerprint, LLM calls) always</li> <li>Queue estimates \u2014 position, avg duration, window-aware estimated start/complete</li> </ul> <p>Infrastructure: +1 Redis container (~50 MB RAM), +1 worker process (~100\u2013200 MB).</p>"},{"location":"sprints/sprint-2/#ar-4-merkle-fingerprints","title":"AR-4: Merkle Fingerprints","text":"<p>Two-level fingerprint system with bottom-up cascade invalidation:</p> <ul> <li>Material fingerprint (<code>content_fingerprint</code>): <code>sha256(processed_content)</code>. Lazy cached in <code>MaterialEntry</code>.</li> <li>Node fingerprint (<code>node_fingerprint</code>): hash of child material fingerprints + child node fingerprints. Lazy cached in <code>MaterialNode</code>.</li> <li>Course fingerprint: hash of root node fingerprints.</li> </ul> <p>Any modification \u2192 invalidates fingerprints from the change point up to the root. <code>fingerprint: null</code> at any level \u2192 something changed below. Drill-down to the specific material.</p> <p>See ERD for the full schema.</p>"},{"location":"sprints/sprint-2/#ar-5-heavy-steps-extraction-serverless-ready","title":"AR-5: Heavy Steps Extraction (serverless-ready)","text":"<p>Separation into heavy (serverless-ready) and light (on-premise) operations:</p> Heavy (serverless-ready) Light (on-premise) Whisper transcription Merge documents Slide/image \u2192 description (vision) Architect agent (LLM call) PDF OCR Fingerprint calculation Video frame extraction CRUD, status management <p>Each heavy step is an injectable callable with a clean contract. <code>SourceProcessor</code> becomes an orchestrator: prepare input \u2192 call heavy step \u2192 package result. When Lambda arrives \u2014 only change the heavy step implementation.</p>"},{"location":"sprints/sprint-2/#ar-6-structure-generation-per-node-cascading","title":"AR-6: Structure Generation \u2014 per-node, cascading","text":"<p>Generation can be triggered for any level of the tree. Cascades through the entire subtree from the target node down.</p> <p>Two modes:</p> <ul> <li>\"free\" \u2014 methodologist builds optimal structure freely. Input tree is context only.</li> <li>\"guided\" \u2014 methodologist preserves input tree as constraint, enriches it.</li> </ul> <p>Cascade logic: find stale materials \u2192 enqueue ingestion jobs \u2192 enqueue structure generation with <code>depends_on</code> \u2192 return 202 Accepted with plan + estimates.</p> <p>Conflict detection: 409 Conflict only when a new generation overlaps with an active job's subtree scope.</p> <p>Idempotency: same <code>(node_id, fingerprint, mode)</code> \u2192 200 OK with existing snapshot.</p> <p>Apply snapshot \u2192 normalized tables: When a snapshot is \"applied\", its <code>structure</code> JSONB is unpacked into <code>modules</code> \u2192 <code>lessons</code> \u2192 <code>concepts</code> \u2192 <code>exercises</code>. <code>Module.snapshot_id</code> FK explicitly links the active structure to the source snapshot.</p>"},{"location":"sprints/sprint-2/#ar-7-slidevideomapping-explicit-references-deferred-validation","title":"AR-7: SlideVideoMapping \u2014 explicit references + deferred validation","text":"<p>Mapping links a specific presentation to a specific video via FK to <code>MaterialEntry</code>. Three-level validation:</p> <ol> <li>Structural (always) \u2014 both materials exist and belong to the node, correct <code>source_type</code>, valid timecode format</li> <li>Content (when READY) \u2014 slide number within presentation range, timecode within video duration</li> <li>Deferred (when not READY) \u2014 mapping created with <code>pending_validation</code> + <code>blocking_factors</code> JSONB; auto-revalidated when blocking material completes ingestion</li> </ol> <p>Batch upload supports partial success \u2014 per-item results with errors, hints, and resubmit guidance.</p>"},{"location":"sprints/sprint-2/#target-api","title":"Target API","text":""},{"location":"sprints/sprint-2/#material-tree-management","title":"Material Tree Management","text":"<pre><code>POST   /api/v1/courses                                     \u2192 create course\nGET    /api/v1/courses                                     \u2192 list courses (pagination)\nGET    /api/v1/courses/{course_id}                         \u2192 course + tree + statuses + fingerprints\nDELETE /api/v1/courses/{course_id}                         \u2192 delete course (cascade)\n\nPOST   /api/v1/courses/{id}/nodes                          \u2192 create root node\nPOST   /api/v1/courses/{id}/nodes/{node_id}/children       \u2192 create child node\nPATCH  /api/v1/courses/{id}/nodes/{node_id}                \u2192 update (title, description, order, parent_id)\nDELETE /api/v1/courses/{id}/nodes/{node_id}                \u2192 delete (cascade children + materials)\n\nPOST   /api/v1/courses/{id}/nodes/{node_id}/materials      \u2192 add material (file or URL)\nDELETE /api/v1/courses/{id}/materials/{material_id}         \u2192 delete material\nPOST   /api/v1/courses/{id}/materials/{material_id}/retry   \u2192 retry ingestion\n\nFile type validation per source_type:\n  video:        .mp4, .webm, .mkv, .avi (or URL)\n  presentation: .pdf, .pptx (or URL)\n  text:         .md, .markdown, .docx, .html, .htm, .txt (or URL)\n  web:          URL only (no file upload)\n</code></pre>"},{"location":"sprints/sprint-2/#slide-video-mapping","title":"Slide-Video Mapping","text":"<pre><code>POST   /api/v1/courses/{id}/nodes/{node_id}/slide-mapping  \u2192 batch create (partial success)\nGET    /api/v1/courses/{id}/nodes/{node_id}/slide-mapping   \u2192 list mappings for node\nDELETE /api/v1/courses/{id}/slide-mapping/{mapping_id}      \u2192 delete mapping\n</code></pre>"},{"location":"sprints/sprint-2/#structure-generation","title":"Structure Generation","text":"<pre><code>POST   /api/v1/courses/{id}/structure/generate             \u2192 trigger for entire course\nPOST   /api/v1/courses/{id}/nodes/{node_id}/structure/generate \u2192 trigger for subtree\n\n         body: { \"mode\": \"free\" | \"guided\" }\n\n         \u2190 200 OK:        snapshot with this fingerprint+mode already exists\n         \u2190 202 Accepted:  job created (with ingestion plan + estimate)\n         \u2190 409 Conflict:  active job overlaps with requested scope\n         \u2190 422 Unprocessable: no READY materials in scope\n\nGET    /api/v1/courses/{id}/structure                      \u2192 latest snapshot (course-level)\nGET    /api/v1/courses/{id}/nodes/{node_id}/structure       \u2192 latest snapshot (node-level)\n</code></pre>"},{"location":"sprints/sprint-2/#jobs-reports","title":"Jobs &amp; Reports","text":"<pre><code>GET    /api/v1/jobs/{job_id}                               \u2192 status of any job\nGET    /api/v1/reports/cost                                \u2192 LLM cost report\nGET    /health                                             \u2192 deep health (DB + S3 + Redis)\n</code></pre>"},{"location":"sprints/sprint-2/#epics","title":"Epics","text":""},{"location":"sprints/sprint-2/#epic-0-project-documentation-infrastructure-1-2-days-complete","title":"Epic 0: Project Documentation Infrastructure (1-2 days) -- COMPLETE","text":"<p>Docs site on GitHub Pages (mkdocs). ERD, sprint descriptions. Executed first \u2014 all subsequent epics are documented in this system.</p>"},{"location":"sprints/sprint-2/#epic-1-infrastructure-arq-redis-4-5-days-complete","title":"Epic 1: Infrastructure -- ARQ + Redis (4-5 days) -- COMPLETE","text":"<p>Task queue with persistence, concurrency control, work window, job tracking, estimates. Redis container + ARQ worker in docker-compose (dev and prod).</p> <p>Completed tasks: S2-001 (Redis in docker-compose), S2-002 (ARQ worker setup), S2-003 (worker config), S2-004 (WorkWindow service), S2-005 (job priorities), S2-006 (Job ORM + repository), S2-007 (queue estimate service), S2-008 (replace BackgroundTasks with ARQ enqueue), S2-009 (ingestion completion callback), S2-010 (IngestionCallback integration), S2-011 (Job API endpoint), S2-012 (worker integration tests).</p>"},{"location":"sprints/sprint-2/#epic-2-materialtree-materialentry-4-5-days-complete","title":"Epic 2: MaterialTree + MaterialEntry (4-5 days) -- COMPLETE","text":"<p>Recursive tree of nodes, MaterialEntry with raw/processed separation and pending receipt. Tree API endpoints, course detail with full tree.</p> <p>Completed tasks: S2-013 (MaterialNode ORM), S2-014 (MaterialNodeRepository), S2-015 (MaterialEntry ORM), S2-016 (MaterialEntryRepository), S2-017 (Node API endpoints), S2-018 (Node API tests), S2-019 (Job API tests), S2-020 (Materials endpoint refactor), S2-021 (Course detail with tree), S2-022 (List courses endpoint), S2-023 (Tree + MaterialEntry unit tests).</p>"},{"location":"sprints/sprint-2/#epic-3-merkle-fingerprints-23-days-complete","title":"Epic 3: Merkle Fingerprints (2\u20133 days) -- COMPLETE","text":"<p>Lazy cached fingerprints with bottom-up cascade invalidation. Material \u2192 node \u2192 course level. Auto-invalidation on CRUD operations.</p> <p>Completed tasks: S2-024 (material-level fingerprint), S2-025 (node-level Merkle hash), S2-026 (course-level fingerprint), S2-027 (cascade invalidation), S2-028 (repository integration), S2-029 (fingerprint in API responses), S2-030 (fingerprint unit tests).</p>"},{"location":"sprints/sprint-2/#epic-4-heavy-steps-extraction-23-days","title":"Epic 4: Heavy Steps Extraction (2\u20133 days)","text":"<p>Injectable heavy operations, serverless-ready boundary. Processors become orchestrators with DI.</p>"},{"location":"sprints/sprint-2/#epic-5-slidevideomapping-redesign-34-days","title":"Epic 5: SlideVideoMapping Redesign (3\u20134 days)","text":"<p>Explicit presentation \u2194 video FK references, three-level validation, deferred validation with auto-revalidation on ingestion complete.</p>"},{"location":"sprints/sprint-2/#epic-6-structure-generation-pipeline-34-days","title":"Epic 6: Structure Generation Pipeline (3\u20134 days)","text":"<p>Per-node trigger, cascading processing, fingerprint check, snapshot persistence, conflict detection, free/guided modes.</p>"},{"location":"sprints/sprint-2/#epic-7-integration-documentation-12-days","title":"Epic 7: Integration Documentation (1\u20132 days)","text":"<p>Flow guide, API reference, auth guide, error handling guide. Published on docs site. Final polish + ops/infrastructure documentation.</p>"},{"location":"sprints/sprint-2/#epic-dependencies","title":"Epic Dependencies","text":"<pre><code>Epic 0 (Docs) \u2500\u2500\u2500 FIRST \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                            \u2502\nEpic 1 (Queue) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n                                        \u2502                   \u2502\nEpic 2 (MaterialTree) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502\n                                        \u2502                   \u2502\nEpic 3 (Fingerprints) \u2500\u2500 Epic 2 \u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502\n                                        \u251c\u2500\u2500\u2192 Epic 6 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\nEpic 4 (Heavy Steps) \u2500\u2500 Epic 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u2502             \u2502\n                                        \u2502      \u2514\u2500\u2500\u2192 Epic 7 \u2500\u2518\nEpic 5 (SlideVideoMapping) \u2500\u2500 E1+E2 \u2500\u2500\u2518\n</code></pre> <p>Recommended order:</p> <ol> <li>Epic 0 (Docs) \u2014 first, 1\u20132 days</li> <li>Epic 1 (Queue) + Epic 2 (MaterialTree) \u2014 in parallel</li> <li>Epic 3 (Fingerprints) \u2014 after Epic 2</li> <li>Epic 4 (Heavy Steps) \u2014 in parallel with Epic 3</li> <li>Epic 5 (SlideVideoMapping) \u2014 after Epic 1 + Epic 2</li> <li>Epic 6 (Structure Generation) \u2014 after all previous</li> <li>Epic 7 (Integration Documentation) \u2014 in parallel with Epic 6</li> </ol>"},{"location":"sprints/sprint-2/#new-dependencies","title":"New Dependencies","text":"<pre><code>[project]\ndependencies = [\n    # ... existing ...\n    \"arq&gt;=0.26\",          # task queue\n    \"redis[hiredis]&gt;=5\",  # ARQ backend + fast connection\n]\n\n[dependency-groups]\ndocs = [\n    \"mkdocs-material&gt;=9\",\n    \"mkdocs-mermaid2-plugin&gt;=1\",\n    \"mkdocs-panzoom-plugin&gt;=0.4\",\n]\n</code></pre>"},{"location":"sprints/sprint-2/#database-changes","title":"Database Changes","text":"<p>See ERD for the full schema diagram.</p> <p>New tables: <code>material_nodes</code>, <code>material_entries</code>, <code>jobs</code>, redesigned <code>slide_video_mappings</code>, <code>course_structure_snapshots</code>.</p> <p>Existing table changes: <code>modules</code> gets <code>snapshot_id</code> FK.</p> <p>Migration strategy: drop old test data + create new tables (no production data to preserve).</p>"}]}